

[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `PaperSearch` with `chain of thought prompting`


[0m[36;1m[1;3m[{'id': 'http://arxiv.org/abs/2201.11903v6', 'updated': '2023-01-10T23:07:57Z', 'published': '2022-01-28T02:33:07Z', 'title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models', 'summary': 'We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.', 'author': [{'name': 'Jason Wei'}, {'name': 'Xuezhi Wang'}, {'name': 'Dale Schuurmans'}, {'name': 'Maarten Bosma'}, {'name': 'Brian Ichter'}, {'name': 'Fei Xia'}, {'name': 'Ed Chi'}, {'name': 'Quoc Le'}, {'name': 'Denny Zhou'}], 'link': [{'@href': 'http://arxiv.org/abs/2201.11903v6', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2201.11903v6', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2311.09277v1', 'updated': '2023-11-15T18:54:01Z', 'published': '2023-11-15T18:54:01Z', 'title': 'Contrastive Chain-of-Thought Prompting', 'summary': 'Despite the success of chain of thought in enhancing language model\nreasoning, the underlying process remains less well understood. Although\nlogically sound reasoning appears inherently crucial for chain of thought,\nprior studies surprisingly reveal minimal impact when using invalid\ndemonstrations instead. Furthermore, the conventional chain of thought does not\ninform language models on what mistakes to avoid, which potentially leads to\nmore errors. Hence, inspired by how humans can learn from both positive and\nnegative examples, we propose contrastive chain of thought to enhance language\nmodel reasoning. Compared to the conventional chain of thought, our approach\nprovides both valid and invalid reasoning demonstrations, to guide the model to\nreason step-by-step while reducing reasoning mistakes. To improve\ngeneralization, we introduce an automatic method to construct contrastive\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\ncontrastive chain of thought can serve as a general enhancement of\nchain-of-thought prompting.', 'author': [{'name': 'Yew Ken Chia'}, {'name': 'Guizhen Chen'}, {'name': 'Luu Anh Tuan'}, {'name': 'Soujanya Poria'}, {'name': 'Lidong Bing'}], 'link': [{'@href': 'http://arxiv.org/abs/2311.09277v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2311.09277v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2502.08092v2', 'updated': '2025-06-02T14:02:08Z', 'published': '2025-02-12T03:33:06Z', 'title': 'GCoT: Chain-of-Thought Prompt Learning for Graphs', 'summary': "Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach.", 'author': [{'name': 'Xingtong Yu'}, {'name': 'Chang Zhou'}, {'name': 'Zhongwei Kuai'}, {'name': 'Xinming Zhang'}, {'name': 'Yuan Fang'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted by SIGKDD2025'}, 'link': [{'@href': 'http://arxiv.org/abs/2502.08092v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2502.08092v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2404.04538v1', 'updated': '2024-04-06T07:39:44Z', 'published': '2024-04-06T07:39:44Z', 'title': 'Soft-Prompting with Graph-of-Thought for Multi-modal Representation\n  Learning', 'summary': 'The chain-of-thought technique has been received well in multi-modal tasks.\nIt is a step-by-step linear reasoning process that adjusts the length of the\nchain to improve the performance of generated prompts. However, human thought\nprocesses are predominantly non-linear, as they encompass multiple aspects\nsimultaneously and employ dynamic adjustment and updating mechanisms.\nTherefore, we propose a novel Aggregation-Graph-of-Thought (AGoT) mechanism for\nsoft-prompt tuning in multi-modal representation learning. The proposed AGoT\nmodels the human thought process not only as a chain but also models each step\nas a reasoning aggregation graph to cope with the overlooked multiple aspects\nof thinking in single-step reasoning. This turns the entire reasoning process\ninto prompt aggregation and prompt flow operations. Experiments show that our\nmulti-modal model enhanced with AGoT soft-prompting achieves good results in\nseveral tasks such as text-image retrieval, visual question answering, and\nimage recognition. In addition, we demonstrate that it has good domain\ngeneralization performance due to better reasoning.', 'author': [{'name': 'Juncheng Yang'}, {'name': 'Zuchao Li'}, {'name': 'Shuai Xie'}, {'name': 'Wei Yu'}, {'name': 'Shijun Li'}, {'name': 'Bo Du'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'This paper is accepted to LREC-COLING 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2404.04538v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2404.04538v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2506.02878v1', 'updated': '2025-06-03T13:45:01Z', 'published': '2025-06-03T13:45:01Z', 'title': 'CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A\n  Theory Perspective', 'summary': 'Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.', 'author': [{'name': 'Jintian Shao'}, {'name': 'Yiming Cheng'}], 'link': [{'@href': 'http://arxiv.org/abs/2506.02878v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2506.02878v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2309.16797v1', 'updated': '2023-09-28T19:01:07Z', 'published': '2023-09-28T19:01:07Z', 'title': 'Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution', 'summary': 'Popular prompt strategies like Chain-of-Thought Prompting can dramatically\nimprove the reasoning abilities of Large Language Models (LLMs) in various\ndomains. However, such hand-crafted prompt-strategies are often sub-optimal. In\nthis paper, we present Promptbreeder, a general-purpose self-referential\nself-improvement mechanism that evolves and adapts prompts for a given domain.\nDriven by an LLM, Promptbreeder mutates a population of task-prompts, and\nsubsequently evaluates them for fitness on a training set. Crucially, the\nmutation of these task-prompts is governed by mutation-prompts that the LLM\ngenerates and improves throughout evolution in a self-referential way. That is,\nPromptbreeder is not just improving task-prompts, but it is also improving the\nmutationprompts that improve these task-prompts. Promptbreeder outperforms\nstate-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve\nPrompting on commonly used arithmetic and commonsense reasoning benchmarks.\nFurthermore, Promptbreeder is able to evolve intricate task-prompts for the\nchallenging problem of hate speech classification.', 'author': [{'name': 'Chrisantha Fernando'}, {'name': 'Dylan Banarse'}, {'name': 'Henryk Michalewski'}, {'name': 'Simon Osindero'}, {'name': 'Tim Rockt√§schel'}], 'link': [{'@href': 'http://arxiv.org/abs/2309.16797v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2309.16797v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2306.00550v1', 'updated': '2023-06-01T11:11:39Z', 'published': '2023-06-01T11:11:39Z', 'title': 'Chain-Of-Thought Prompting Under Streaming Batch: A Case Study', 'summary': 'Recently, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting\nLLMs in performing complex reasoning. However, developing effective prompts can\nbe a challenging and labor-intensive task. Many studies come out of some way to\nautomatically construct CoT from test data. Most of them assume that all test\ndata is visible before testing and only select a small subset to generate\nrationales, which is an unrealistic assumption. In this paper, we present a\ncase study on how to construct and optimize chain-of-thought prompting using\nbatch data in streaming settings.', 'author': {'name': 'Yuxin Tang'}, 'link': [{'@href': 'http://arxiv.org/abs/2306.00550v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2306.00550v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2403.14982v2', 'updated': '2024-04-03T07:40:07Z', 'published': '2024-03-22T06:31:49Z', 'title': 'MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of\n  Chain-of-Thoughts', 'summary': 'Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -\nwhich provides a dataset of puzzles for testing natural language understanding.\nWe employ large language models (LLMs) to solve this task through several\nprompting techniques. Zero-shot and few-shot prompting generate reasonably good\nresults when tested with proprietary LLMs, compared to the open-source models.\nWe obtain further improved results with chain-of-thought prompting, an\niterative prompting method that breaks down the reasoning process step-by-step.\nWe obtain our best results by utilizing an ensemble of chain-of-thought\nprompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle\nsubtask. The strong performance of prompted LLMs demonstrates their capability\nfor complex reasoning when provided with a decomposition of the thought\nprocess. Our work sheds light on how step-wise explanatory prompts can unlock\nmore of the knowledge encoded in the parameters of large models.', 'author': [{'name': 'Md Nishat Raihan'}, {'name': 'Dhiman Goswami'}, {'name': 'Al Nahian Bin Emran'}, {'name': 'Sadiya Sayara Chowdhury Puspo'}, {'name': 'Amrita Ganguly'}, {'name': 'Marcos Zampieri'}], 'link': [{'@href': 'http://arxiv.org/abs/2403.14982v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2403.14982v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2302.12822v3', 'updated': '2024-02-27T14:49:43Z', 'published': '2023-02-24T18:58:06Z', 'title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from\n  Labeled Data', 'summary': 'Chain-of-thought (CoT) advances the reasoning abilities of large language\nmodels (LLMs) and achieves superior performance in complex reasoning tasks.\nHowever, most CoT studies rely on carefully designed human-annotated rational\nchains to prompt LLMs, posing challenges for real-world applications where\nlabeled data is available without rational chains. This paper proposes a new\nstrategy, Automate-CoT (Automatic Prompt Augmentation and Selection with\nChain-of-Thought), that can bypass human engineering of CoT by automatically\naugmenting rational chains from a small labeled dataset, and then pruning\nlow-quality chains to construct a candidate pool of machine-generated rationale\nchains based on the labels. Finally, it selects the optimal combination of\nseveral rationale chains from the pool for CoT prompting by employing a\nvariance-reduced policy gradient strategy to estimate the significance of each\nexample. Automate-CoT enables a quick adaptation of the CoT technique to\ndifferent tasks. Experimental results demonstrate the effectiveness of our\nmethod, where competitive results are achieved on arithmetic reasoning (+2.7%),\ncommonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning\ntasks (+2.5%). The code is available at\nhttps://github.com/SHUMKASHUN/Automate-CoT.', 'author': [{'name': 'KaShun Shum'}, {'name': 'Shizhe Diao'}, {'name': 'Tong Zhang'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'EMNLP 2023'}, 'link': [{'@href': 'http://arxiv.org/abs/2302.12822v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2302.12822v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2501.08641v1', 'updated': '2025-01-15T08:07:22Z', 'published': '2025-01-15T08:07:22Z', 'title': 'Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights\n  and Limitations', 'summary': "The relationship between language and thought remains an unresolved\nphilosophical issue. Existing viewpoints can be broadly categorized into two\nschools: one asserting their independence, and another arguing that language\nconstrains thought. In the context of large language models, this debate raises\na crucial question: Does a language model's grasp of semantic meaning depend on\nthought processes? To explore this issue, we investigate whether reasoning\ntechniques can facilitate semantic understanding. Specifically, we\nconceptualize thought as reasoning, employ chain-of-thought prompting as a\nreasoning technique, and examine its impact on sentiment analysis tasks. The\nexperiments show that chain-of-thought has a minimal impact on sentiment\nanalysis tasks. Both the standard and chain-of-thought prompts focus on aspect\nterms rather than sentiment in the generated content. Furthermore,\ncounterfactual experiments reveal that the model's handling of sentiment tasks\nprimarily depends on information from demonstrations. The experimental results\nsupport the first viewpoint.", 'author': [{'name': 'Kaiyuan Zheng'}, {'name': 'Qinghua Zhao'}, {'name': 'Lei Li'}], 'link': [{'@href': 'http://arxiv.org/abs/2501.08641v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2501.08641v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2505.04135v1', 'updated': '2025-05-07T05:13:15Z', 'published': '2025-05-07T05:13:15Z', 'title': 'Enhancing Granular Sentiment Classification with Chain-of-Thought\n  Prompting in Large Language Models', 'summary': "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance.", 'author': [{'name': 'Vihaan Miriyala'}, {'name': 'Smrithi Bukkapatnam'}, {'name': 'Lavanya Prahallad'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '5 pages'}, 'link': [{'@href': 'http://arxiv.org/abs/2505.04135v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2505.04135v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2408.13372v1', 'updated': '2024-08-23T21:10:09Z', 'published': '2024-08-23T21:10:09Z', 'title': 'Understanding Defects in Generated Codes by Language Models', 'summary': "This study investigates the reliability of code generation by Large Language\nModels (LLMs), focusing on identifying and analyzing defects in the generated\ncode. Despite the advanced capabilities of LLMs in automating code generation,\nensuring the accuracy and functionality of the output remains a significant\nchallenge. By using a structured defect classification method to understand\ntheir nature and origins this study categorizes and analyzes 367 identified\ndefects from code snippets generated by LLMs, with a significant proportion\nbeing functionality and algorithm errors. These error categories indicate key\nareas where LLMs frequently fail, underscoring the need for targeted\nimprovements. To enhance the accuracy of code generation, this paper\nimplemented five prompt engineering techniques, including Scratchpad Prompting,\nProgram of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code\nPrompting, and Structured Chain-of-Thought Prompting. These techniques were\napplied to refine the input prompts, aiming to reduce ambiguities and improve\nthe models' accuracy rate. The research findings suggest that precise and\nstructured prompting significantly mitigates common defects, thereby increasing\nthe reliability of LLM-generated code.", 'author': [{'name': 'Ali Mohammadi Esfahani'}, {'name': 'Nafiseh Kahani'}, {'name': 'Samuel A. Ajila'}], 'link': [{'@href': 'http://arxiv.org/abs/2408.13372v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2408.13372v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2503.07932v1', 'updated': '2025-03-11T00:21:32Z', 'published': '2025-03-11T00:21:32Z', 'title': 'A Theory of Learning with Autoregressive Chain of Thought', 'summary': 'For a given base class of sequence-to-next-token generators, we consider\nlearning prompt-to-answer mappings obtained by iterating a fixed,\ntime-invariant generator for multiple steps, thus generating a\nchain-of-thought, and then taking the final token as the answer. We formalize\nthe learning problems both when the chain-of-thought is observed and when\ntraining only on prompt-answer pairs, with the chain-of-thought latent. We\nanalyze the sample and computational complexity both in terms of general\nproperties of the base class (e.g. its VC dimension) and for specific base\nclasses such as linear thresholds. We present a simple base class that allows\nfor universal representability and computationally tractable chain-of-thought\nlearning. Central to our development is that time invariance allows for sample\ncomplexity that is independent of the length of the chain-of-thought. Attention\narises naturally in our construction.', 'author': [{'name': 'Nirmit Joshi'}, {'name': 'Gal Vardi'}, {'name': 'Adam Block'}, {'name': 'Surbhi Goel'}, {'name': 'Zhiyuan Li'}, {'name': 'Theodor Misiakiewicz'}, {'name': 'Nathan Srebro'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Comments are welcome'}, 'link': [{'@href': 'http://arxiv.org/abs/2503.07932v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2503.07932v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CC', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2306.14050v2', 'updated': '2024-04-15T21:58:27Z', 'published': '2023-06-24T20:15:07Z', 'title': 'Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think"\n  Step-by-Step', 'summary': 'Chain-of-thought prompting (e.g., "Let\'s think step-by-step") primes large\nlanguage models to verbalize rationalization for their predictions. While\nchain-of-thought can lead to dramatic performance gains, benefits appear to\nemerge only for sufficiently large models (beyond 50B parameters). We show that\norders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit\nfrom chain-of-thought prompting. To achieve this, we introduce Symbolic\nChain-of-Thought Distillation (SCoTD), a method to train a smaller student\nmodel on rationalizations sampled from a significantly larger teacher model.\nExperiments across several commonsense benchmarks show that: 1) SCoTD enhances\nthe performance of the student model in both supervised and few-shot settings,\nand especially for challenge sets; 2) sampling many reasoning chains per\ninstance from the teacher is paramount; and 3) after distillation, student\nchain-of-thoughts are judged by humans as comparable to the teacher, despite\norders of magnitude fewer parameters. We test several hypotheses regarding what\nproperties of chain-of-thought samples are important, e.g., diversity vs.\nteacher likelihood vs. open-endedness. We release our corpus of\nchain-of-thought samples and code.', 'author': [{'name': 'Liunian Harold Li'}, {'name': 'Jack Hessel'}, {'name': 'Youngjae Yu'}, {'name': 'Xiang Ren'}, {'name': 'Kai-Wei Chang'}, {'name': 'Yejin Choi'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ACL 2023'}, 'link': [{'@href': 'http://arxiv.org/abs/2306.14050v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2306.14050v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2501.02152v1', 'updated': '2025-01-04T00:58:06Z', 'published': '2025-01-04T00:58:06Z', 'title': 'Table as Thought: Exploring Structured Thoughts in LLM Reasoning', 'summary': "Large language models' reasoning abilities benefit from methods that organize\ntheir thought processes, such as chain-of-thought prompting, which employs a\nsequential structure to guide the reasoning process step-by-step. However,\nexisting approaches focus primarily on organizing the sequence of thoughts,\nleaving structure in individual thought steps underexplored. To address this\ngap, we propose Table as Thought, a framework inspired by cognitive\nneuroscience theories on human thought. Table as Thought organizes reasoning\nwithin a tabular schema, where rows represent sequential thought steps and\ncolumns capture critical constraints and contextual information to enhance\nreasoning. The reasoning process iteratively populates the table until\nself-verification ensures completeness and correctness. Our experiments show\nthat Table as Thought excels in planning tasks and demonstrates a strong\npotential for enhancing LLM performance in mathematical reasoning compared to\nunstructured thought baselines. This work provides a novel exploration of\nrefining thought representation within LLMs, paving the way for advancements in\nreasoning and AI cognition.", 'author': [{'name': 'Zhenjie Sun'}, {'name': 'Naihao Deng'}, {'name': 'Haofei Yu'}, {'name': 'Jiaxuan You'}], 'link': [{'@href': 'http://arxiv.org/abs/2501.02152v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2501.02152v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2503.21464v1', 'updated': '2025-03-27T12:54:00Z', 'published': '2025-03-27T12:54:00Z', 'title': 'Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial\n  Prompt Detection', 'summary': 'In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.', 'author': [{'name': 'Ryan Marinelli'}, {'name': 'Josef Pichlmeier'}, {'name': 'Tamas Bisztray'}], 'link': [{'@href': 'http://arxiv.org/abs/2503.21464v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2503.21464v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.PF', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2412.20545v1', 'updated': '2024-12-29T18:34:10Z', 'published': '2024-12-29T18:34:10Z', 'title': 'The Impact of Prompt Programming on Function-Level Code Generation', 'summary': 'Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. Despite this, the impact\nof different prompt techniques -- and their combinations -- on code generation\nremains underexplored. In this study, we introduce CodePromptEval, a dataset of\n7072 prompts designed to evaluate five prompt techniques (few-shot, persona,\nchain-of-thought, function signature, list of packages) and their effect on the\ncorrectness, similarity, and quality of complete functions generated by three\nLLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt\ntechniques significantly influence the generated code, combining multiple\ntechniques does not necessarily improve the outcome. Additionally, we observed\na trade-off between correctness and quality when using prompt techniques. Our\ndataset and replication package enable future research on improving\nLLM-generated code and evaluating new prompt techniques.', 'author': [{'name': 'Ranim Khojah'}, {'name': 'Francisco Gomes de Oliveira Neto'}, {'name': 'Mazen Mohamad'}, {'name': 'Philipp Leitner'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'CodePromptEval dataset and replication package on GitHub:\n  https://github.com/icetlab/CodePromptEval'}, 'link': [{'@href': 'http://arxiv.org/abs/2412.20545v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2412.20545v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2305.09993v2', 'updated': '2024-05-23T21:23:12Z', 'published': '2023-05-17T06:35:43Z', 'title': 'Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs\n  Sampling', 'summary': 'We introduce Reprompting, an iterative sampling algorithm that automatically\nlearns the Chain-of-Thought (CoT) recipes for a given task without human\nintervention. Through Gibbs sampling, Reprompting infers the CoT recipes that\nwork consistently well for a set of training samples by iteratively sampling\nnew recipes using previously sampled recipes as parent prompts to solve other\ntraining problems. We conduct extensive experiments on 20 challenging reasoning\ntasks. Results show that Reprompting outperforms human-written CoT prompts\nsubstantially by +9.4 points on average. It also achieves consistently better\nperformance than the state-of-the-art prompt optimization and decoding\nalgorithms.', 'author': [{'name': 'Weijia Xu'}, {'name': 'Andrzej Banburski-Fahey'}, {'name': 'Nebojsa Jojic'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ICML 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2305.09993v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.09993v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2304.07919v2', 'updated': '2023-06-17T06:40:27Z', 'published': '2023-04-16T23:59:25Z', 'title': 'Chain of Thought Prompt Tuning in Vision Language Models', 'summary': 'Language-Image Pre-training has demonstrated promising results on zero-shot\nand few-shot downstream tasks by prompting visual models with natural language\nprompts. However, most recent studies only use a single prompt for tuning,\nneglecting the inherent step-to-step cognitive reasoning process that humans\nconduct in complex task settings, for example, when processing images from\nunfamiliar domains. Chain of Thought is a simple and effective approximation to\nhuman reasoning process and has been proven useful for natural language\nprocessing (NLP) tasks. Based on this cognitive intuition, we believe that\nconducting effective reasoning is also an important problem in visual tasks,\nand a chain of thought could be a solution to this problem. In this work, we\npropose a novel chain of thought prompt tuning for vision-language modeling.\nExtensive experiments show that our method not only generalizes better in image\nclassification tasks, has greater transferability beyond a single dataset, and\nhas stronger domain generalization performance, but also performs much better\nin imagetext retrieval and visual question answering, which require more\nreasoning capabilities. We are the first to successfully adapt chain-of-thought\nprompting that combines visual and textual embeddings. We will release our\ncodes', 'author': [{'name': 'Jiaxin Ge'}, {'name': 'Hongyin Luo'}, {'name': 'Siyuan Qian'}, {'name': 'Yulu Gan'}, {'name': 'Jie Fu'}, {'name': 'Shanghang Zhang'}], 'link': [{'@href': 'http://arxiv.org/abs/2304.07919v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2304.07919v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2310.16069v2', 'updated': '2023-10-26T12:35:37Z', 'published': '2023-10-24T13:32:32Z', 'title': 'CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought\n  Language Prompting', 'summary': 'Natural scene analysis and remote sensing imagery offer immense potential for\nadvancements in large-scale language-guided context-aware data utilization.\nThis potential is particularly significant for enhancing performance in\ndownstream tasks such as object detection and segmentation with designed\nlanguage prompting. In light of this, we introduce the CPSeg, Chain-of-Thought\nLanguage Prompting for Finer-grained Semantic Segmentation), an innovative\nframework designed to augment image segmentation performance by integrating a\nnovel "Chain-of-Thought" process that harnesses textual information associated\nwith images. This groundbreaking approach has been applied to a flood disaster\nscenario. CPSeg encodes prompt texts derived from various sentences to\nformulate a coherent chain-of-thought. We propose a new vision-language\ndataset, FloodPrompt, which includes images, semantic masks, and corresponding\ntext information. This not only strengthens the semantic understanding of the\nscenario but also aids in the key task of semantic segmentation through an\ninterplay of pixel and text matching maps. Our qualitative and quantitative\nanalyses validate the effectiveness of CPSeg.', 'author': {'name': 'Lei Li'}, 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'WACV 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2310.16069v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2310.16069v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2403.02738v3', 'updated': '2024-12-17T16:10:26Z', 'published': '2024-03-05T07:47:34Z', 'title': 'Causal Prompting: Debiasing Large Language Model Prompting based on\n  Front-Door Adjustment', 'summary': 'Despite the notable advancements of existing prompting methods, such as\nIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they\nstill face challenges related to various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including approaches based on data\naugmentation and reweighting, yet they struggle with the complex biases\ninherent in LLMs. To address such limitations, the causal relationship behind\nthe prompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate LLMs biases. In specific, causal intervention is achieved\nby designing the prompts without accessing the parameters and logits of LLMs.\nThe chain-of-thought generated by LLM is employed as the mediator variable and\nthe causal effect between input prompts and output answers is calculated\nthrough front-door adjustment to mitigate model biases. Moreover, to accurately\nrepresent the chain-of-thoughts and estimate the causal effects, contrastive\nlearning is used to fine-tune the encoder of chain-of-thought by aligning its\nspace with that of the LLM. Experimental results show that the proposed causal\nprompting approach achieves excellent performance across seven natural language\nprocessing datasets on both open-source and closed-source LLMs.', 'author': [{'name': 'Congzhi Zhang'}, {'name': 'Linhai Zhang'}, {'name': 'Jialong Wu'}, {'name': 'Yulan He'}, {'name': 'Deyu Zhou'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted by AAAI 2025'}, 'link': [{'@href': 'http://arxiv.org/abs/2403.02738v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2403.02738v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2311.09773v1', 'updated': '2023-11-16T10:55:29Z', 'published': '2023-11-16T10:55:29Z', 'title': 'To be or not to be? an exploration of continuously controllable prompt\n  engineering', 'summary': 'As the use of large language models becomes more widespread, techniques like\nparameter-efficient fine-tuning and other methods for controlled generation are\ngaining traction for customizing models and managing their outputs. However,\nthe challenge of precisely controlling how prompts influence these models is an\narea ripe for further investigation. In response, we introduce ControlPE\n(Continuously Controllable Prompt Engineering). ControlPE enables finer\nadjustments to prompt effects, complementing existing prompt engineering, and\neffectively controls continuous targets. This approach harnesses the power of\nLoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,\nenabling fine-tuned adjustments to the impact of prompts. Our methodology\ninvolves generating specialized datasets for prompt distillation, incorporating\nthese prompts into the LoRA model, and carefully adjusting LoRA merging weight\nto regulate the influence of prompts. This provides a dynamic and adaptable\ntool for prompt control. Through our experiments, we have validated the\npracticality and efficacy of ControlPE. It proves to be a promising solution\nfor control a variety of prompts, ranging from generating short responses\nprompts, refusal prompts to chain-of-thought prompts.', 'author': [{'name': 'Yuhan Sun'}, {'name': 'Mukai Li'}, {'name': 'Yixin Cao'}, {'name': 'Kun Wang'}, {'name': 'Wenxiao Wang'}, {'name': 'Xingyu Zeng'}, {'name': 'Rui Zhao'}], 'link': [{'@href': 'http://arxiv.org/abs/2311.09773v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2311.09773v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2310.04959v1', 'updated': '2023-10-08T01:16:55Z', 'published': '2023-10-08T01:16:55Z', 'title': 'Towards Better Chain-of-Thought Prompting Strategies: A Survey', 'summary': 'Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its\nimpressive strength when used as a prompting strategy for large language models\n(LLM). Recent years, the prominent effect of CoT prompting has attracted\nemerging research. However, there still lacks of a systematic summary about key\nfactors of CoT prompting and comprehensive guide for prompts utilizing. For a\ndeeper understanding about CoT prompting, we survey on a wide range of current\nresearch, presenting a systematic and comprehensive analysis on several factors\nthat may influence the effect of CoT prompting, and introduce how to better\napply it in different applications under these discussions. We further analyze\nthe challenges and propose some future directions about CoT prompting. This\nsurvey could provide an overall reference on related research.', 'author': [{'name': 'Zihan Yu'}, {'name': 'Liang He'}, {'name': 'Zhen Wu'}, {'name': 'Xinyu Dai'}, {'name': 'Jiajun Chen'}], 'link': [{'@href': 'http://arxiv.org/abs/2310.04959v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2310.04959v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2305.06599v3', 'updated': '2023-09-07T11:39:07Z', 'published': '2023-05-11T06:43:37Z', 'title': 'Structured Chain-of-Thought Prompting for Code Generation', 'summary': 'Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive\nperformance in code generation. LLMs take prompts as inputs, and\nChain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.\nCoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural\nlanguage reasoning steps) and then output the code. However, CoT prompting is\ndesigned for natural language generation and has low accuracy in code\ngeneration.\n  In this paper, we propose Structured CoTs (SCoTs) and present a novel\nprompting technique for code generation, named SCoT prompting. Our motivation\nis source code contains rich structural information and any code can be\ncomposed of three program structures (i.e., sequence, branch, and loop\nstructures). Intuitively, structured intermediate reasoning steps make for\nstructured source code. Thus, we ask LLMs to use program structures to build\nCoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.\nCompared to CoT prompting, SCoT prompting explicitly constrains LLMs to think\nabout how to solve requirements from the view of source code and further the\nperformance of LLMs in code generation. We apply SCoT prompting to two LLMs\n(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,\nMBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline\n- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human\ndevelopers prefer programs from SCoT prompting. (3) SCoT prompting is robust to\nexamples and achieves substantial improvements.', 'author': [{'name': 'Jia Li'}, {'name': 'Ge Li'}, {'name': 'Yongmin Li'}, {'name': 'Zhi Jin'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'arXiv admin note: text overlap with arXiv:2303.17780'}, 'link': [{'@href': 'http://arxiv.org/abs/2305.06599v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.06599v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.11140v2', 'updated': '2025-01-06T21:18:24Z', 'published': '2024-02-17T00:13:36Z', 'title': 'Boosting of Thoughts: Trial-and-Error Problem Solving with Large\n  Language Models', 'summary': 'The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches.', 'author': [{'name': 'Sijia Chen'}, {'name': 'Baochun Li'}, {'name': 'Di Niu'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted as a poster paper by ICLR2024. 27 pages, 5 figures, 18\n  tables. [Source\n  Code](https://github.com/iQua/llmpebase/tree/main/examples/BoTReasoning)'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.11140v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.11140v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2504.02323v2', 'updated': '2025-04-09T23:50:32Z', 'published': '2025-04-03T06:53:34Z', 'title': 'CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning,\n  and Active Learning for Generalizable Formative Assessment Scoring', 'summary': "Large language models (LLMs) have created new opportunities to assist\nteachers and support student learning. Methods such as chain-of-thought (CoT)\nprompting enable LLMs to grade formative assessments in science, providing\nscores and relevant feedback to students. However, the extent to which these\nmethods generalize across curricula in multiple domains (such as science,\ncomputing, and engineering) remains largely untested. In this paper, we\nintroduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based\napproach to formative assessment scoring that (1) leverages Evidence-Centered\nDesign (ECD) principles to develop curriculum-aligned formative assessments and\nrubrics, (2) applies human-in-the-loop prompt engineering to automate response\nscoring, and (3) incorporates teacher and student feedback to iteratively\nrefine assessment questions, grading rubrics, and LLM prompts for automated\ngrading. Our findings demonstrate that CoTAL improves GPT-4's scoring\nperformance, achieving gains of up to 24.5% over a non-prompt-engineered\nbaseline. Both teachers and students view CoTAL as effective in scoring and\nexplaining student responses, each providing valuable refinements to enhance\ngrading accuracy and explanation quality.", 'author': [{'name': 'Clayton Cohn'}, {'name': 'Nicole Hutchins'}, {'name': 'Ashwin T S'}, {'name': 'Gautam Biswas'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Submitted to IEEE Transactions on Learning Technologies. Currently\n  under review'}, 'link': [{'@href': 'http://arxiv.org/abs/2504.02323v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2504.02323v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2304.11657v3', 'updated': '2024-03-15T10:28:13Z', 'published': '2023-04-23T13:54:39Z', 'title': 'Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in\n  Large Language Models', 'summary': "Large language models (LLMs) can achieve highly effective performance on\nvarious reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting as demonstrations. However, the reasoning chains of demonstrations\ngenerated by LLMs are prone to errors, which can subsequently lead to incorrect\nreasoning during inference. Furthermore, inappropriate exemplars (overly\nsimplistic or complex), can affect overall performance among varying levels of\ndifficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts\nPrompting), an iterative bootstrapping approach for selecting exemplars and\ngenerating reasoning chains. By utilizing iterative bootstrapping, our approach\nenables LLMs to autonomously rectify errors, resulting in more precise and\ncomprehensive reasoning chains. Simultaneously, our approach selects\nchallenging yet answerable questions accompanied by reasoning chains as\nexemplars with a moderate level of difficulty, which enhances the LLMs'\ngeneralizability across varying levels of difficulty. Experimental results\nindicate that Iter-CoT exhibits superiority, achieving competitive performance\nacross three distinct reasoning tasks on ten datasets.", 'author': [{'name': 'Jiashuo Sun'}, {'name': 'Yi Luo'}, {'name': 'Yeyun Gong'}, {'name': 'Chen Lin'}, {'name': 'Yelong Shen'}, {'name': 'Jian Guo'}, {'name': 'Nan Duan'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted by NAACL 2024 Findings'}, 'link': [{'@href': 'http://arxiv.org/abs/2304.11657v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2304.11657v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.05376v1', 'updated': '2024-02-08T03:17:38Z', 'published': '2024-02-08T03:17:38Z', 'title': 'Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms\n  in Large Language Models', 'summary': 'Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks and exhibited impressive reasoning abilities by applying\nzero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature\nof sentence prefixes during the pre-training phase, existing zero-shot CoT\nprompting methods that employ identical CoT prompting across all task instances\nmay not be optimal. In this paper, we introduce a novel zero-shot prompting\nmethod that leverages evolutionary algorithms to generate diverse promptings\nfor LLMs dynamically. Our approach involves initializing two CoT promptings,\nperforming evolutionary operations based on LLMs to create a varied set, and\nutilizing the LLMs to select a suitable CoT prompting for a given problem.\nAdditionally, a rewriting operation, guided by the selected CoT prompting,\nenhances the understanding of the LLMs about the problem. Extensive experiments\nconducted across ten reasoning datasets demonstrate the superior performance of\nour proposed method compared to current zero-shot CoT prompting methods on\nGPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore\nthe adaptability and effectiveness of our method in various reasoning tasks.', 'author': [{'name': 'Feihu Jin'}, {'name': 'Yifan Liu'}, {'name': 'Ying Tan'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '17 pages, 5 figures, 16 tables'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.05376v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.05376v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2311.01460v1', 'updated': '2023-11-02T17:59:49Z', 'published': '2023-11-02T17:59:49Z', 'title': 'Implicit Chain of Thought Reasoning via Knowledge Distillation', 'summary': 'To augment language models with the ability to reason, researchers usually\nprompt or finetune them to produce chain of thought reasoning steps before\nproducing the final answer. However, although people use natural language to\nreason effectively, it may be that LMs could reason more effectively with some\nintermediate computation that is not in natural language. In this work, we\nexplore an alternative reasoning approach: instead of explicitly producing the\nchain of thought reasoning steps, we use the language model\'s internal hidden\nstates to perform implicit reasoning. The implicit reasoning steps are\ndistilled from a teacher model trained on explicit chain-of-thought reasoning,\nand instead of doing reasoning "horizontally" by producing intermediate words\none-by-one, we distill it such that the reasoning happens "vertically" among\nthe hidden states in different layers. We conduct experiments on a multi-digit\nmultiplication task and a grade school math problem dataset and find that this\napproach enables solving tasks previously not solvable without explicit\nchain-of-thought, at a speed comparable to no chain-of-thought.', 'author': [{'name': 'Yuntian Deng'}, {'name': 'Kiran Prasad'}, {'name': 'Roland Fernandez'}, {'name': 'Paul Smolensky'}, {'name': 'Vishrav Chaudhary'}, {'name': 'Stuart Shieber'}], 'link': [{'@href': 'http://arxiv.org/abs/2311.01460v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2311.01460v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2310.14799v1', 'updated': '2023-10-23T10:56:03Z', 'published': '2023-10-23T10:56:03Z', 'title': 'Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning\n  across Languages', 'summary': 'Chain-of-thought (CoT) is capable of eliciting models to explicitly generate\nreasoning paths, thus promoting reasoning accuracy and attracting increasing\nattention. Specifically, zero-shot CoT achieves remarkable improvements in a\nwide range of reasoning tasks by simply instructing the LLM with the prompt\n"Let\'s think step by step!". Despite the success of zero-shot CoT, the existing\nzero-shot prompting techniques remain limited to a single language, making it\nchallenging to generalize to other languages and hindering global development.\nIn this work, we introduce cross-lingual prompting (CLP), aiming to improve\nzero-shot CoT reasoning across languages. Specifically, CLP consists of two\nmain components: (1) cross-lingual alignment prompting and (2) task-specific\nsolver prompting. The cross-lingual alignment prompting is responsible for\naligning representations across different languages, whereas the task-specific\nsolver prompting is used to generate the final chain of thoughts and results\nfor the reasoning task. In addition, we further introduce cross-lingual\nself-consistent prompting (CLSP) to ensemble different reasoning paths across\nlanguages. Our experimental evaluations on several benchmarks demonstrate that\nCLP and CLSP significantly outperform the existing prompting methods and\nachieve state-of-the-art performance. We hope this work will inspire further\nbreakthroughs in cross-lingual CoT.', 'author': [{'name': 'Libo Qin'}, {'name': 'Qiguang Chen'}, {'name': 'Fuxuan Wei'}, {'name': 'Shijue Huang'}, {'name': 'Wanxiang Che'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted at EMNLP2023 Main Conference'}, 'link': [{'@href': 'http://arxiv.org/abs/2310.14799v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2310.14799v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.10645v3', 'updated': '2024-10-09T06:54:29Z', 'published': '2024-02-16T12:46:16Z', 'title': 'Can Separators Improve Chain-of-Thought Prompting?', 'summary': "Chain-of-thought (CoT) prompting is a simple and effective method for\nimproving the reasoning capabilities of Large Language Models (LLMs). The basic\nidea of CoT is to let LLMs break down their thought processes step-by-step by\nputting exemplars in the input prompt. However, the densely structured prompt\nexemplars of CoT may cause the cognitive overload of LLMs. Inspired by human\ncognition, we introduce COT-SEP, a method that strategically employs separators\nat the end of each exemplar in CoT prompting. These separators are designed to\nhelp the LLMs understand their thought processes better while reasoning.\nInterestingly, it turns out that COT-SEP significantly improves the LLMs'\nperformances on complex reasoning tasks (e.g., GSM8K, AQuA, CSQA), compared\nwith the vanilla CoT, which does not use separators. We also study the effects\nof the type and the location of separators tested on multiple LLMs, including\nGPT-3.5-Turbo, GPT-4, and LLaMA-2 7B.", 'author': [{'name': 'Yoonjeong Park'}, {'name': 'Hyunjin Kim'}, {'name': 'Chanyeol Choi'}, {'name': 'Junseong Kim'}, {'name': 'Jy-yong Sohn'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'IEEE FLLM 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.10645v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.10645v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2308.10444v1', 'updated': '2023-08-21T03:31:20Z', 'published': '2023-08-21T03:31:20Z', 'title': 'Dynamic Strategy Chain: Dynamic Zero-Shot CoT for Long Mental Health\n  Support Generation', 'summary': "Long counseling Text Generation for Mental health support (LTGM), an\ninnovative and challenging task, aims to provide help-seekers with mental\nhealth support through a comprehensive and more acceptable response. The\ncombination of chain-of-thought (CoT) prompting and Large Language Models\n(LLMs) is employed and get the SOTA performance on various NLP tasks,\nespecially on text generation tasks. Zero-shot CoT prompting is one of the most\ncommon methods in CoT prompting. However, in the LTGM task, Zero-shot CoT\nprompting can not simulate a counselor or provide personalized strategies\nwithout effective mental health counseling strategy prompts. To tackle this\nchallenge, we propose a zero-shot Dynamic Strategy Chain (DSC) prompting\nmethod. Firstly, we utilize GPT2 to learn the responses written by mental\nhealth counselors and dynamically generate mental health counseling strategies\ntailored to the help-seekers' needs. Secondly, the Zero-shot DSC prompting is\nconstructed according to mental health counseling strategies and the\nhelp-seekers' post. Finally, the Zero-shot DSC prompting is employed to guide\nLLMs in generating more human-like responses for the help-seekers. Both\nautomatic and manual evaluations demonstrate that Zero-shot DSC prompting can\ndeliver more human-like responses than CoT prompting methods on LTGM tasks.", 'author': [{'name': 'Qi Chen'}, {'name': 'Dexi Liu'}], 'link': [{'@href': 'http://arxiv.org/abs/2308.10444v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2308.10444v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2305.04091v3', 'updated': '2023-05-26T07:06:48Z', 'published': '2023-05-06T16:34:37Z', 'title': 'Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning\n  by Large Language Models', 'summary': 'Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n"Let\'s think step by step" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.', 'author': [{'name': 'Lei Wang'}, {'name': 'Wanyu Xu'}, {'name': 'Yihuai Lan'}, {'name': 'Zhiqiang Hu'}, {'name': 'Yunshi Lan'}, {'name': 'Roy Ka-Wei Lee'}, {'name': 'Ee-Peng Lim'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ACL 2023'}, 'link': [{'@href': 'http://arxiv.org/abs/2305.04091v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.04091v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2302.00618v1', 'updated': '2023-02-01T17:33:12Z', 'published': '2023-02-01T17:33:12Z', 'title': 'Synthetic Prompting: Generating Chain-of-Thought Demonstrations for\n  Large Language Models', 'summary': 'Large language models can perform various reasoning tasks by using\nchain-of-thought prompting, which guides them to find answers through\nstep-by-step demonstrations. However, the quality of the prompts depends on the\ndemonstrations given to the models, and creating many of them by hand is\ncostly. We introduce Synthetic prompting, a method that leverages a few\nhandcrafted examples to prompt the model to generate more examples by itself,\nand selects effective demonstrations to elicit better reasoning. Our method\nalternates between a backward and forward process to generate new examples. The\nbackward process generates a question that match a sampled reasoning chain, so\nthat the question is solvable and clear. The forward process produces a more\ndetailed reasoning chain for the question, improving the quality of the\nexample. We evaluate our method on numerical, symbolic, and algorithmic\nreasoning tasks, and show that it outperforms existing prompting techniques.', 'author': [{'name': 'Zhihong Shao'}, {'name': 'Yeyun Gong'}, {'name': 'Yelong Shen'}, {'name': 'Minlie Huang'}, {'name': 'Nan Duan'}, {'name': 'Weizhu Chen'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Preprint'}, 'link': [{'@href': 'http://arxiv.org/abs/2302.00618v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2302.00618v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2203.11171v4', 'updated': '2023-03-07T17:57:37Z', 'published': '2022-03-21T17:48:52Z', 'title': 'Self-Consistency Improves Chain of Thought Reasoning in Language Models', 'summary': 'Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).', 'author': [{'name': 'Xuezhi Wang'}, {'name': 'Jason Wei'}, {'name': 'Dale Schuurmans'}, {'name': 'Quoc Le'}, {'name': 'Ed Chi'}, {'name': 'Sharan Narang'}, {'name': 'Aakanksha Chowdhery'}, {'name': 'Denny Zhou'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Published at ICLR 2023. V2: added PaLM results; V3: added UL2\n  results; V4: camera ready version at ICLR 2023'}, 'link': [{'@href': 'http://arxiv.org/abs/2203.11171v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2203.11171v4', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2407.03600v2', 'updated': '2024-08-27T08:00:03Z', 'published': '2024-07-04T03:20:31Z', 'title': 'Chain-of-Thought Augmentation with Logit Contrast for Enhanced Reasoning\n  in Language Models', 'summary': 'Rapidly increasing model scales coupled with steering methods such as\nchain-of-thought prompting have led to drastic improvements in language model\nreasoning. At the same time, models struggle with compositional generalization\nand are far from human performance on many reasoning-based benchmarks.\nLeveraging the success of chain-of-thought prompting, and also taking\ninspiration from context-aware decoding (CAD), we explore input-based\ncontrasting methods to further encourage the type of reasoning induced by\nchain-of-thought prompting. While work remains to stabilize these results\nacross datasets and models, the improvements we find warrant further\ninvestigation into input-based steering methods for context-aware reasoning.', 'author': [{'name': 'Jay Shim'}, {'name': 'Grant Kruttschnitt'}, {'name': 'Alyssa Ma'}, {'name': 'Daniel Kim'}, {'name': 'Benjamin Chek'}, {'name': 'Athul Anand'}, {'name': 'Kevin Zhu'}, {'name': "Sean O'Brien"}], 'link': [{'@href': 'http://arxiv.org/abs/2407.03600v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2407.03600v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2403.03544v1', 'updated': '2024-03-06T08:43:30Z', 'published': '2024-03-06T08:43:30Z', 'title': 'Prompt Mining for Language-based Human Mobility Forecasting', 'summary': 'With the advancement of large language models, language-based forecasting has\nrecently emerged as an innovative approach for predicting human mobility\npatterns. The core idea is to use prompts to transform the raw mobility data\ngiven as numerical values into natural language sentences so that the language\nmodels can be leveraged to generate the description for future observations.\nHowever, previous studies have only employed fixed and manually designed\ntemplates to transform numerical values into sentences. Since the forecasting\nperformance of language models heavily relies on prompts, using fixed templates\nfor prompting may limit the forecasting capability of language models. In this\npaper, we propose a novel framework for prompt mining in language-based\nmobility forecasting, aiming to explore diverse prompt design strategies.\nSpecifically, the framework includes a prompt generation stage based on the\ninformation entropy of prompts and a prompt refinement stage to integrate\nmechanisms such as the chain of thought. Experimental results on real-world\nlarge-scale data demonstrate the superiority of generated prompts from our\nprompt mining pipeline. Additionally, the comparison of different prompt\nvariants shows that the proposed prompt refinement process is effective. Our\nstudy presents a promising direction for further advancing language-based\nmobility forecasting.', 'author': [{'name': 'Hao Xue'}, {'name': 'Tianye Tang'}, {'name': 'Ali Payani'}, {'name': 'Flora D. Salim'}], 'link': [{'@href': 'http://arxiv.org/abs/2403.03544v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2403.03544v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2311.03033v1', 'updated': '2023-11-06T11:13:17Z', 'published': '2023-11-06T11:13:17Z', 'title': 'Beyond Words: A Mathematical Framework for Interpreting Large Language\n  Models', 'summary': 'Large language models (LLMs) are powerful AI tools that can generate and\ncomprehend natural language text and other complex information. However, the\nfield lacks a mathematical framework to systematically describe, compare and\nimprove LLMs. We propose Hex a framework that clarifies key terms and concepts\nin LLM research, such as hallucinations, alignment, self-verification and\nchain-of-thought reasoning. The Hex framework offers a precise and consistent\nway to characterize LLMs, identify their strengths and weaknesses, and\nintegrate new findings. Using Hex, we differentiate chain-of-thought reasoning\nfrom chain-of-thought prompting and establish the conditions under which they\nare equivalent. This distinction clarifies the basic assumptions behind\nchain-of-thought prompting and its implications for methods that use it, such\nas self-verification and prompt programming.\n  Our goal is to provide a formal framework for LLMs that can help both\nresearchers and practitioners explore new possibilities for generative AI. We\ndo not claim to have a definitive solution, but rather a tool for opening up\nnew research avenues. We argue that our formal definitions and results are\ncrucial for advancing the discussion on how to build generative AI systems that\nare safe, reliable, fair and robust, especially in domains like healthcare and\nsoftware engineering.', 'author': [{'name': 'Javier Gonz√°lez'}, {'name': 'Aditya V. Nori'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '4 figures, 18 pages'}, 'link': [{'@href': 'http://arxiv.org/abs/2311.03033v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2311.03033v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2504.20769v1', 'updated': '2025-04-29T13:50:05Z', 'published': '2025-04-29T13:50:05Z', 'title': 'Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in\n  Large Language Models against Reference Corruption', 'summary': 'Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%.', 'author': [{'name': 'Wenxiao Wang'}, {'name': 'Parsa Hosseini'}, {'name': 'Soheil Feizi'}], 'link': [{'@href': 'http://arxiv.org/abs/2504.20769v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2504.20769v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.11863v3', 'updated': '2024-04-01T08:33:11Z', 'published': '2024-02-19T06:11:28Z', 'title': 'How Interpretable are Reasoning Explanations from Prompting Large\n  Language Models?', 'summary': 'Prompt Engineering has garnered significant attention for enhancing the\nperformance of large language models across a multitude of tasks. Techniques\nsuch as the Chain-of-Thought not only bolster task performance but also\ndelineate a clear trajectory of reasoning steps, offering a tangible form of\nexplanation for the audience. Prior works on interpretability assess the\nreasoning chains yielded by Chain-of-Thought solely along a singular axis,\nnamely faithfulness. We present a comprehensive and multifaceted evaluation of\ninterpretability, examining not only faithfulness but also robustness and\nutility across multiple commonsense reasoning benchmarks. Likewise, our\ninvestigation is not confined to a single prompting technique; it expansively\ncovers a multitude of prevalent prompting techniques employed in large language\nmodels, thereby ensuring a wide-ranging and exhaustive evaluation. In addition,\nwe introduce a simple interpretability alignment technique, termed\nSelf-Entailment-Alignment Chain-of-thought, that yields more than 70\\%\nimprovements across multiple dimensions of interpretability. Code is available\nat https://github.com/SenticNet/CoT_interpretability', 'author': [{'name': 'Wei Jie Yeo'}, {'name': 'Ranjan Satapathy'}, {'name': 'Rick Siow Mong Goh'}, {'name': 'Erik Cambria'}], 'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '10.18653/v1/2024.findings-naacl.138'}, 'link': [{'@title': 'doi', '@href': 'http://dx.doi.org/10.18653/v1/2024.findings-naacl.138', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2402.11863v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.11863v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'NAACL Findings 2024'}, 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2207.00747v1', 'updated': '2022-07-02T06:20:57Z', 'published': '2022-07-02T06:20:57Z', 'title': 'Rationale-Augmented Ensembles in Language Models', 'summary': 'Recent research has shown that rationales, or step-by-step chains of thought,\ncan be used to improve performance in multi-step reasoning tasks. We reconsider\nrationale-augmented prompting for few-shot in-context learning, where (input ->\noutput) prompts are expanded to (input, rationale -> output) prompts. For\nrationale-augmented prompting we demonstrate how existing approaches, which\nrely on manual prompt engineering, are subject to sub-optimal rationales that\nmay harm performance. To mitigate this brittleness, we propose a unified\nframework of rationale-augmented ensembles, where we identify rationale\nsampling in the output space as the key component to robustly improve\nperformance. This framework is general and can easily be extended to common\nnatural language processing tasks, even those that do not traditionally\nleverage intermediate steps, such as question answering, word sense\ndisambiguation, and sentiment analysis. We demonstrate that rationale-augmented\nensembles achieve more accurate and interpretable results than existing\nprompting approaches--including standard prompting without rationales and\nrationale-based chain-of-thought prompting--while simultaneously improving\ninterpretability of model predictions through the associated rationales.', 'author': [{'name': 'Xuezhi Wang'}, {'name': 'Jason Wei'}, {'name': 'Dale Schuurmans'}, {'name': 'Quoc Le'}, {'name': 'Ed Chi'}, {'name': 'Denny Zhou'}], 'link': [{'@href': 'http://arxiv.org/abs/2207.00747v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2207.00747v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2412.01690v1', 'updated': '2024-12-02T16:34:18Z', 'published': '2024-12-02T16:34:18Z', 'title': 'Can We Afford The Perfect Prompt? Balancing Cost and Accuracy with the\n  Economical Prompting Index', 'summary': 'As prompt engineering research rapidly evolves, evaluations beyond accuracy\nare crucial for developing cost-effective techniques. We present the Economical\nPrompting Index (EPI), a novel metric that combines accuracy scores with token\nconsumption, adjusted by a user-specified cost concern level to reflect\ndifferent resource constraints. Our study examines 6 advanced prompting\ntechniques, including Chain-of-Thought, Self-Consistency, and Tree of Thoughts,\nacross 10 widely-used language models and 4 diverse datasets. We demonstrate\nthat approaches such as Self-Consistency often provide statistically\ninsignificant gains while becoming cost-prohibitive. For example, on\nhigh-performing models like Claude 3.5 Sonnet, the EPI of simpler techniques\nlike Chain-of-Thought (0.72) surpasses more complex methods like\nSelf-Consistency (0.64) at slight cost concern levels. Our findings suggest a\nreevaluation of complex prompting strategies in resource-constrained scenarios,\npotentially reshaping future research priorities and improving\ncost-effectiveness for end-users.', 'author': [{'name': 'Tyler McDonald'}, {'name': 'Anthony Colosimo'}, {'name': 'Yifeng Li'}, {'name': 'Ali Emami'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '5 pages (excluding references), accepted to Coling 2025'}, 'link': [{'@href': 'http://arxiv.org/abs/2412.01690v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2412.01690v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2304.11556v1', 'updated': '2023-04-23T06:52:35Z', 'published': '2023-04-23T06:52:35Z', 'title': 'Divide and Prompt: Chain of Thought Prompting for Text-to-SQL', 'summary': "Chain-of-thought (CoT) prompting combined with large language models (LLMs)\nhave achieved encouraging results on complex reasoning tasks. Text-to-SQL is a\ncritical semantic parsing task that converts natural language questions into\nSQL statements, involving a complex reasoning process. However, there is little\nwork about using CoT prompting to activate LLM's reasoning capabilities on\nText-to-SQL tasks. In this work, we propose a new paradigm for prompting\nText-to-SQL tasks, called Divide-and-Prompt, which first divides the task into\nsubtasks, and then approach each subtask through CoT. We present 3\nprompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments\nshow that these prompts guide LLMs to generate Text-to-SQL with higher\nexecution accuracy.", 'author': [{'name': 'Xiping Liu'}, {'name': 'Zhao Tan'}], 'link': [{'@href': 'http://arxiv.org/abs/2304.11556v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2304.11556v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2405.04776v3', 'updated': '2025-03-12T04:56:46Z', 'published': '2024-05-08T02:48:28Z', 'title': 'Chain of Thoughtlessness? An Analysis of CoT in Planning', 'summary': "Large language model (LLM) performance on reasoning problems typically does\nnot generalize out of distribution. Previous work has claimed that this can be\nmitigated with chain of thought prompting-a method of demonstrating solution\nprocedures-with the intuition that it is possible to in-context teach an LLM an\nalgorithm for solving the problem. This paper presents a case study of chain of\nthought on problems from Blocksworld, a classical planning domain, and examines\nthe performance of two state-of-the-art LLMs across two axes: generality of\nexamples given in prompt, and complexity of problems queried with each prompt.\nWhile our problems are very simple, we only find meaningful performance\nimprovements from chain of thought prompts when those prompts are exceedingly\nspecific to their problem class, and that those improvements quickly\ndeteriorate as the size n of the query-specified stack grows past the size of\nstacks shown in the examples. We also create scalable variants of three domains\ncommonly studied in previous CoT papers and demonstrate the existence of\nsimilar failure modes. Our results hint that, contrary to previous claims in\nthe literature, CoT's performance improvements do not stem from the model\nlearning general algorithmic procedures via demonstrations but depend on\ncarefully engineering highly problem specific prompts. This spotlights\ndrawbacks of chain of thought, especially the sharp tradeoff between possible\nperformance gains and the amount of human labor necessary to generate examples\nwith correct reasoning traces.", 'author': [{'name': 'Kaya Stechly'}, {'name': 'Karthik Valmeekam'}, {'name': 'Subbarao Kambhampati'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'NeurIPS 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2405.04776v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2405.04776v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2401.05787v2', 'updated': '2025-03-17T10:35:11Z', 'published': '2024-01-11T09:49:15Z', 'title': 'Chain of Evidences and Evidence to Generate: Prompting for Context\n  Grounded and Retrieval Augmented Reasoning', 'summary': 'While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)\nsuffer from limitations like limited context grounding,\nhallucination/inconsistent output generation, and iterative sluggishness. To\novercome these challenges, we introduce a novel mono/dual-step zero-shot\nprompting framework built upon two unique strategies Chain of Evidences (CoE)}\nand Evidence to Generate (E2G). Instead of unverified reasoning claims, our\ninnovative approaches leverage the power of "evidence for decision making" by\nfirst focusing exclusively on the thought sequences explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM\'s output\ngeneration process with greater precision and efficiency. This simple yet\npotent approach unlocks the full potential of chain-of-thoughts prompting,\nfacilitating faster, more reliable, and contextually aware reasoning in LLMs.\nOur framework consistently achieves remarkable results across various\nknowledge-intensive reasoning and generation tasks, surpassing baseline\napproaches with state-of-the-art LLMs. For instance, (i) on the LogiQA\nbenchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,\nsurpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2\noutperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,\nachieving an F1 score of 83.3 on DROP. We release our prompts and outputs on\nthese benchmarks as a new instruction tuning dataset for future research at\nhttps://huggingface.co/datasets/kagnlp/Chain-of-Evidences/.', 'author': {'name': 'Md Rizwan Parvez'}, 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted at NAACL KnowledgeNLP 2025'}, 'link': [{'@href': 'http://arxiv.org/abs/2401.05787v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2401.05787v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2401.14423v4', 'updated': '2024-05-05T00:54:26Z', 'published': '2024-01-24T06:20:18Z', 'title': 'Prompt Design and Engineering: Introduction and Advanced Methods', 'summary': 'Prompt design and engineering has rapidly become essential for maximizing the\npotential of large language models. In this paper, we introduce core concepts,\nadvanced techniques like Chain-of-Thought and Reflection, and the principles\nbehind building LLM-based agents. Finally, we provide a survey of tools for\nprompt engineers.', 'author': {'name': 'Xavier Amatriain'}, 'link': [{'@href': 'http://arxiv.org/abs/2401.14423v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2401.14423v4', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SE', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2408.07676v1', 'updated': '2024-08-14T17:23:12Z', 'published': '2024-08-14T17:23:12Z', 'title': 'Enhanced Detection of Conversational Mental Manipulation Through\n  Advanced Prompting Techniques', 'summary': 'This study presents a comprehensive, long-term project to explore the\neffectiveness of various prompting techniques in detecting dialogical mental\nmanipulation. We implement Chain-of-Thought prompting with Zero-Shot and\nFew-Shot settings on a binary mental manipulation detection task, building upon\nexisting work conducted with Zero-Shot and Few- Shot prompting. Our primary\nobjective is to decipher why certain prompting techniques display superior\nperformance, so as to craft a novel framework tailored for detection of mental\nmanipulation. Preliminary findings suggest that advanced prompting techniques\nmay not be suitable for more complex models, if they are not trained through\nexample-based learning.', 'author': [{'name': 'Ivory Yang'}, {'name': 'Xiaobo Guo'}, {'name': 'Sean Xie'}, {'name': 'Soroush Vosoughi'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted at WiNLP @ EMNLP 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2408.07676v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2408.07676v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2309.02045v2', 'updated': '2024-01-07T14:59:15Z', 'published': '2023-09-05T08:44:23Z', 'title': 'Enhance Multi-domain Sentiment Analysis of Review Texts through\n  Prompting Strategies', 'summary': "Large Language Models (LLMs) have made significant strides in both scientific\nresearch and practical applications. Existing studies have demonstrated the\nstate-of-the-art (SOTA) performance of LLMs in various natural language\nprocessing tasks. However, the question of how to further enhance LLMs'\nperformance in specific task using prompting strategies remains a pivotal\nconcern. This paper explores the enhancement of LLMs' performance in sentiment\nanalysis through the application of prompting strategies. We formulate the\nprocess of prompting for sentiment analysis tasks and introduce two novel\nstrategies tailored for sentiment analysis: RolePlaying (RP) prompting and\nChain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT\nprompting strategy which is a combination of RP prompting and CoT prompting. We\nconduct comparative experiments on three distinct domain datasets to evaluate\nthe effectiveness of the proposed sentiment analysis strategies. The results\ndemonstrate that the adoption of the proposed prompting strategies leads to a\nincreasing enhancement in sentiment analysis accuracy. Further, the CoT\nprompting strategy exhibits a notable impact on implicit sentiment analysis,\nwith the RP-CoT prompting strategy delivering the most superior performance\namong all strategies.", 'author': [{'name': 'Yajing Wang'}, {'name': 'Zongwei Luo'}], 'link': [{'@href': 'http://arxiv.org/abs/2309.02045v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2309.02045v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2305.18507v2', 'updated': '2023-10-07T08:07:46Z', 'published': '2023-05-29T15:14:09Z', 'title': 'Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models', 'summary': 'Large language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.', 'author': [{'name': 'Yi Hu'}, {'name': 'Haotong Yang'}, {'name': 'Zhouchen Lin'}, {'name': 'Muhan Zhang'}], 'link': [{'@href': 'http://arxiv.org/abs/2305.18507v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.18507v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2305.10601v2', 'updated': '2023-12-03T22:50:35Z', 'published': '2023-05-17T23:16:17Z', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'summary': "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.", 'author': [{'name': 'Shunyu Yao'}, {'name': 'Dian Yu'}, {'name': 'Jeffrey Zhao'}, {'name': 'Izhak Shafran'}, {'name': 'Thomas L. Griffiths'}, {'name': 'Yuan Cao'}, {'name': 'Karthik Narasimhan'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'NeurIPS 2023 camera ready version. Code repo with all prompts:\n  https://github.com/princeton-nlp/tree-of-thought-llm'}, 'link': [{'@href': 'http://arxiv.org/abs/2305.10601v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.10601v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2410.23856v1', 'updated': '2024-10-31T12:07:44Z', 'published': '2024-10-31T12:07:44Z', 'title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?', 'summary': "This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.", 'author': [{'name': 'Zhanke Zhou'}, {'name': 'Rong Tao'}, {'name': 'Jianing Zhu'}, {'name': 'Yiwen Luo'}, {'name': 'Zengmao Wang'}, {'name': 'Bo Han'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted by NeurIPS 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2410.23856v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2410.23856v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2501.12226v1', 'updated': '2025-01-21T15:51:07Z', 'published': '2025-01-21T15:51:07Z', 'title': 'CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning', 'summary': 'Large Language Models (LLMs) have recently achieved impressive results in\ncomplex reasoning tasks through Chain of Thought (CoT) prompting. However, most\nexisting CoT methods rely on using the same prompts, whether manually designed\nor automatically generated, to handle the entire dataset. This\none-size-fits-all approach may fail to meet the specific needs arising from the\ndiversities within a single dataset. To solve this problem, we propose the\nClustered Distance-Weighted Chain of Thought (CDW-CoT) method, which\ndynamically constructs prompts tailored to the characteristics of each data\ninstance by integrating clustering and prompt optimization techniques. Our\nmethod employs clustering algorithms to categorize the dataset into distinct\ngroups, from which a candidate pool of prompts is selected to reflect the\ninherent diversity within the dataset. For each cluster, CDW-CoT trains the\noptimal prompt probability distribution tailored to their specific\ncharacteristics. Finally, it dynamically constructs a unique prompt probability\ndistribution for each test instance, based on its proximity to cluster centers,\nfrom which prompts are selected for reasoning. CDW-CoT consistently outperforms\ntraditional CoT methods across six datasets, including commonsense, symbolic,\nand mathematical reasoning tasks. Specifically, when compared to manual CoT,\nCDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and\n15.72% on LLaMA3 (8B).', 'author': [{'name': 'Yuanheng Fang'}, {'name': 'Guoqing Chao'}, {'name': 'Wenqiang Lei'}, {'name': 'Shaobo Li'}, {'name': 'Dianhui Chu'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'aaai25(poster)'}, 'link': [{'@href': 'http://arxiv.org/abs/2501.12226v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2501.12226v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2307.13339v1', 'updated': '2023-07-25T08:51:30Z', 'published': '2023-07-25T08:51:30Z', 'title': 'Analyzing Chain-of-Thought Prompting in Large Language Models via\n  Gradient-based Feature Attributions', 'summary': 'Chain-of-thought (CoT) prompting has been shown to empirically improve the\naccuracy of large language models (LLMs) on various question answering tasks.\nWhile understanding why CoT prompting is effective is crucial to ensuring that\nthis phenomenon is a consequence of desired model behavior, little work has\naddressed this; nonetheless, such an understanding is a critical prerequisite\nfor responsible model deployment. We address this question by leveraging\ngradient-based feature attribution methods which produce saliency scores that\ncapture the influence of input tokens on model output. Specifically, we probe\nseveral open-source LLMs to investigate whether CoT prompting affects the\nrelative importances they assign to particular input tokens. Our results\nindicate that while CoT prompting does not increase the magnitude of saliency\nscores attributed to semantically relevant tokens in the prompt compared to\nstandard few-shot prompting, it increases the robustness of saliency scores to\nquestion perturbations and variations in model output.', 'author': [{'name': 'Skyler Wu'}, {'name': 'Eric Meng Shen'}, {'name': 'Charumathi Badrinath'}, {'name': 'Jiaqi Ma'}, {'name': 'Himabindu Lakkaraju'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted to Workshop on Challenges in Deployable Generative AI at\n  ICML 2023'}, 'link': [{'@href': 'http://arxiv.org/abs/2307.13339v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2307.13339v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2305.14215v2', 'updated': '2023-10-27T15:21:38Z', 'published': '2023-05-23T16:32:36Z', 'title': 'Exploring Chain-of-Thought Style Prompting for Text-to-SQL', 'summary': "In-context learning with large language models (LLMs) has recently caught\nincreasing attention due to its superior few-shot performance on various tasks.\nHowever, its performance on text-to-SQL parsing still has much room for\nimprovement. In this paper, we hypothesize that a crucial aspect of LLMs to\nimprove for text-to-SQL parsing is their multi-step reasoning ability. Thus, we\nsystematically study how to enhance LLMs' reasoning ability through chain of\nthought (CoT) style prompting, including the original chain-of-thought\nprompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023).\nOur experiments demonstrate that iterative prompting as in Zhou et al. (2023)\nmay be unnecessary for text-to-SQL parsing, and using detailed reasoning steps\ntends to have more error propagation issues. Based on these findings, we\npropose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2\nand 6.5 point absolute gains on the Spider development set and the Spider\nRealistic set, respectively, compared to the standard prompting method without\nreasoning steps; 2.4 and 1.5 point absolute gains, compared to the\nleast-to-most prompting method.", 'author': [{'name': 'Chang-You Tai'}, {'name': 'Ziru Chen'}, {'name': 'Tianshu Zhang'}, {'name': 'Xiang Deng'}, {'name': 'Huan Sun'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'EMNLP 2023 main; long paper'}, 'link': [{'@href': 'http://arxiv.org/abs/2305.14215v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.14215v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2210.03493v1', 'updated': '2022-10-07T12:28:21Z', 'published': '2022-10-07T12:28:21Z', 'title': 'Automatic Chain of Thought Prompting in Large Language Models', 'summary': 'Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like "Let\'s think step by\nstep" to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the "Let\'s think step by step" prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let\'s think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot', 'author': [{'name': 'Zhuosheng Zhang'}, {'name': 'Aston Zhang'}, {'name': 'Mu Li'}, {'name': 'Alex Smola'}], 'link': [{'@href': 'http://arxiv.org/abs/2210.03493v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2210.03493v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2312.13848v1', 'updated': '2023-12-21T13:45:02Z', 'published': '2023-12-21T13:45:02Z', 'title': 'Reducing Hallucinations: Enhancing VQA for Flood Disaster Damage\n  Assessment with Visual Contexts', 'summary': "The zero-shot performance of visual question answering (VQA) models relies\nheavily on prompts. For example, a zero-shot VQA for disaster scenarios could\nleverage well-designed Chain of Thought (CoT) prompts to stimulate the model's\npotential. However, using CoT prompts has some problems, such as causing an\nincorrect answer in the end due to the hallucination in the thought process. In\nthis paper, we propose a zero-shot VQA named Flood Disaster VQA with Two-Stage\nPrompt (VQA-TSP). The model generates the thought process in the first stage\nand then uses the thought process to generate the final answer in the second\nstage. In particular, visual context is added in the second stage to relieve\nthe hallucination problem that exists in the thought process. Experimental\nresults show that our method exceeds the performance of state-of-the-art\nzero-shot VQA models for flood disaster scenarios in total. Our study provides\na research basis for improving the performance of CoT-based zero-shot VQA.", 'author': [{'name': 'Yimin Sun'}, {'name': 'Chao Wang'}, {'name': 'Yan Peng'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'already be accepted by 2024 3rd International Conference on Computer,\n  Artificial Intelligence and Control Engineering (CAICE 2024)'}, 'link': [{'@href': 'http://arxiv.org/abs/2312.13848v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2312.13848v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2409.17539v2', 'updated': '2025-02-07T01:16:53Z', 'published': '2024-09-26T04:59:45Z', 'title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in\n  Large Language Models', 'summary': "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks but their performance in complex logical reasoning tasks remains\nunsatisfactory. Although some prompting methods, such as Chain-of-Thought, can\nimprove the reasoning ability of LLMs to some extent, they suffer from an\nunfaithful issue where derived conclusions may not align with the generated\nreasoning chain. To address this issue, some studies employ the approach of\npropositional logic to further enhance logical reasoning abilities of LLMs.\nHowever, the potential omissions in the extraction of logical expressions in\nthese methods can cause information loss in the logical reasoning process,\nthereby generating incorrect results. To this end, we propose Logic-of-Thought\n(LoT) prompting which employs propositional logic to generate expanded logical\ninformation descriptions and utilizes them as an additional augmentation to\noriginal contexts, thereby ensuring information completeness and enhancing\nlogical reasoning ability. LoT is orthogonal to existing prompting methods and\ncan be seamlessly integrated with them. Extensive experiments demonstrate that\nLoT boosts the performance of various prompting methods with a striking margin\nacross five logical reasoning tasks. In particular, LoT enhances\nChain-of-Thought's performance on the ReClor dataset by +4.35%, improves\nChain-of-Thought with Self-Consistency's performance on the RuleTaker dataset\nby +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter\ndataset by +8%.", 'author': [{'name': 'Tongxuan Liu'}, {'name': 'Wenjiang Xu'}, {'name': 'Weizhe Huang'}, {'name': 'Yuting Zeng'}, {'name': 'Jiaxing Wang'}, {'name': 'Xingyu Wang'}, {'name': 'Hailong Yang'}, {'name': 'Jing Li'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '18 pages, Accepted to NAACL2025'}, 'link': [{'@href': 'http://arxiv.org/abs/2409.17539v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2409.17539v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2411.05778v2', 'updated': '2024-11-11T21:09:42Z', 'published': '2024-11-08T18:45:06Z', 'title': 'LLMs as Method Actors: A Model for Prompt Engineering and Architecture', 'summary': 'We introduce "Method Actors" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a "Method Actors" approach can\nsignificantly improve LLM performance over both a vanilla and "Chain of\nThoughts" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a "Chain of Thoughts" approach solves 41% of puzzles, whereas our\nstrongest "Method Actor" approach solves 86% of puzzles. We also test OpenAI\'s\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a "Method Actor" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%.', 'author': {'name': 'Colin Doyle'}, 'link': [{'@href': 'http://arxiv.org/abs/2411.05778v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2411.05778v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2209.08141v2', 'updated': '2023-05-19T18:17:24Z', 'published': '2022-09-16T19:23:13Z', 'title': 'Psychologically-informed chain-of-thought prompts for metaphor\n  understanding in large language models', 'summary': 'Probabilistic models of language understanding are valuable tools for\ninvestigating human language use. However, they need to be hand-designed for a\nparticular domain. In contrast, large language models (LLMs) are trained on\ntext that spans a wide array of domains, but they lack the structure and\ninterpretability of probabilistic models. In this paper, we use\nchain-of-thought prompts to introduce structures from probabilistic models into\nLLMs. We explore this approach in the case of metaphor understanding. Our\nchain-of-thought prompts lead language models to infer latent variables and\nreason about their relationships in order to choose appropriate paraphrases for\nmetaphors. The latent variables and relationships chosen are informed by\ntheories of metaphor understanding from cognitive psychology. We apply these\nprompts to the two largest versions of GPT-3 and show that they can improve\nperformance in a paraphrase selection task.', 'author': [{'name': 'Ben Prystawski'}, {'name': 'Paul Thibodeau'}, {'name': 'Christopher Potts'}, {'name': 'Noah D. Goodman'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '7 pages, 1 figure'}, 'link': [{'@href': 'http://arxiv.org/abs/2209.08141v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2209.08141v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2310.16535v1', 'updated': '2023-10-25T10:34:02Z', 'published': '2023-10-25T10:34:02Z', 'title': 'R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought\n  Reasoning in Large Language Models under Noisy Context', 'summary': 'With the help of Chain-of-Thought (CoT) prompting, Large Language Models\n(LLMs) have achieved remarkable performance on various reasoning tasks.\nHowever, most of them have been evaluated under noise-free context and the\ndilemma for LLMs to produce inaccurate results under the noisy context has not\nbeen fully investigated. Existing studies utilize trigger sentences to\nencourage LLMs to concentrate on the relevant information but the trigger has\nlimited effect on final answer prediction. Inspired by interactive CoT method,\nwhere intermediate reasoning steps are promoted by multiple rounds of\ninteraction between users and LLMs, we propose a novel prompting method, namely\nR$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$\nprompting interacts with LLMs to perform key sentence extraction, variable\ndeclaration and answer prediction, which corresponds to a thought process of\nreviewing, rephrasing and resolving. The responses generated at the last\ninteraction will perform as hints to guide toward the responses of the next\ninteraction. Our experiments show that R$^3$ prompting significantly\noutperforms existing CoT prompting methods on five reasoning tasks under noisy\ncontext. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on\nthe reasoning tasks under noisy context compared to the most competitive\nprompting baseline. More analyses and ablation studies show the robustness and\ngeneralization of R$^3$ prompting method in solving reasoning tasks in LLMs\nunder noisy context.', 'author': [{'name': 'Qingyuan Tian'}, {'name': 'Hanlun Zhu'}, {'name': 'Lei Wang'}, {'name': 'Yang Li'}, {'name': 'Yunshi Lan'}], 'link': [{'@href': 'http://arxiv.org/abs/2310.16535v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2310.16535v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.10949v2', 'updated': '2024-02-20T15:03:00Z', 'published': '2024-02-09T22:48:45Z', 'title': 'The Unreasonable Effectiveness of Eccentric Automatic Prompts', 'summary': 'Large Language Models (LLMs) have demonstrated remarkable problem-solving and\nbasic mathematics abilities. However, their efficacy is highly contingent on\nthe formulation of the prompt. This study endeavors to quantify the influence\nof incorporating "positive thinking" into the system message of the prompt,\nthen compare that to systematic prompt optimization. We assess the performance\nof 60 combinations of system message snippets, tested with and without Chain of\nThought prompting, across three models with parameters ranging from 7 to 70\nbillion on the GSM8K dataset. Our findings reveal that results do not\nuniversally generalize across models. In most instances, the inclusion of\n"positive thinking" prompts positively affected model performance. Notably,\nhowever, Llama2-70B exhibited an exception when not utilizing Chain of Thought,\nas the optimal system message was found to be none at all. Given the\ncombinatorial complexity, and thus computation time, of experimenting with\nhand-tuning prompts for large black-box models, we then compared the\nperformance of the best "positive thinking" prompt against the output of\nsystematic prompt optimization. We show that employing an automated prompt\noptimizer emerges as the most effective method for enhancing performance, even\nwhen working with smaller open-source models. Additionally, our findings reveal\nthat the highest-scoring, automatically-optimized prompt exhibits a degree of\npeculiarity far beyond expectations.', 'author': [{'name': 'Rick Battle'}, {'name': 'Teja Gollapudi'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.10949v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.10949v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2410.14198v1', 'updated': '2024-10-18T06:25:27Z', 'published': '2024-10-18T06:25:27Z', 'title': 'Supervised Chain of Thought', 'summary': 'Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n"one-prompt-for-all" approach, using a single prompt structure (e.g., "think\nstep by step") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.', 'author': [{'name': 'Xiang Zhang'}, {'name': 'Dujian Ding'}], 'link': [{'@href': 'http://arxiv.org/abs/2410.14198v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2410.14198v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2409.13697v1', 'updated': '2024-09-04T04:13:16Z', 'published': '2024-09-04T04:13:16Z', 'title': 'Prompt Baking', 'summary': 'Two primary ways to change LLM behavior are prompting and weight updates\n(e.g., fine-tuning). Prompting LLMs is simple and effective, specifying the\ndesired changes explicitly in natural language, whereas weight updates provide\nmore expressive and permanent behavior changes, specified implicitly via\ntraining on large datasets. We present a technique for "baking" prompts into\nthe weights of an LLM. Prompt Baking converts a prompt $u$ and initial weights\n$\\theta$ to a new set of weights $\\theta_u$ such that new "baked" LLM behaves\nlike the original prompted LLM. Mathematically, we minimize the KL divergence\nbetween $P_\\theta(\\cdot | u)$ and $P_{\\theta_u}(\\cdot)$, where $P$ is the LLM\'s\nprobability distribution over token sequences. Across all our experiments, we\nfind prompts can be readily baked into weight updates. Baking chain-of-thought\nprompts improves zero-shot performance on GSM8K, ASDiv, MBPP, ARC-Easy,\nARC-Challenge, and CommonsenseQA benchmarks. Baking news headlines directly\nupdates an LLM\'s knowledge. And baking instructions & personas alleviates\n"prompt forgetting" over long sequences. Furthermore, stopping baking early\ncreates "half-baked" models, continuously scaling prompt strength. Baked models\nretain their sensitivity to further prompting and baking, including\nre-prompting with the baked-in prompt. Surprisingly, the re-prompted models\nyield further performance gains in instruction following, as well as math\nreasoning and coding benchmarks. Taking re-prompting and re-baking to the limit\nyields a form of iterative self-improvement we call Prompt Pursuit, and\npreliminary results on instruction following exhibit dramatic performance\ngains. Finally, we discuss implications for AI safety, continuous model\nupdating, enhancing real-time learning capabilities in LLM-based agents, and\ngenerating more stable AI personas.', 'author': [{'name': 'Aman Bhargava'}, {'name': 'Cameron Witkowski'}, {'name': 'Alexander Detkov'}, {'name': 'Matt Thomson'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '25 pages, 8 figures'}, 'link': [{'@href': 'http://arxiv.org/abs/2409.13697v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2409.13697v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2407.11000v1', 'updated': '2024-06-25T10:14:44Z', 'published': '2024-06-25T10:14:44Z', 'title': 'Autonomous Prompt Engineering in Large Language Models', 'summary': 'Prompt engineering is a crucial yet challenging task for optimizing the\nperformance of large language models (LLMs) on customized tasks. This\npioneering research introduces the Automatic Prompt Engineering Toolbox (APET),\nwhich enables GPT-4 to autonomously apply prompt engineering techniques. By\nleveraging sophisticated strategies such as Expert Prompting, Chain of Thought,\nand Tree of Thoughts, APET empowers GPT-4 to dynamically optimize prompts,\nresulting in substantial improvements in tasks like Word Sorting (4.4%\nincrease) and Geometric Shapes (6.8% increase). Despite encountering challenges\nin complex tasks such as Checkmate in One (-14.8%), these findings demonstrate\nthe transformative potential of APET in automating complex prompt optimization\nprocesses without the use of external data. Overall, this research represents a\nsignificant leap in AI development, presenting a robust framework for future\ninnovations in autonomous AI systems and highlighting the ability of GPT-4 to\nbring prompt engineering theory to practice. It establishes a foundation for\nenhancing performance in complex task performance and broadening the practical\napplications of these techniques in real-world scenarios.', 'author': [{'name': 'Daan Kepel'}, {'name': 'Konstantina Valogianni'}], 'link': [{'@href': 'http://arxiv.org/abs/2407.11000v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2407.11000v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2307.10573v2', 'updated': '2023-07-23T02:58:50Z', 'published': '2023-07-20T04:28:53Z', 'title': 'Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in\n  Language Model Prompting', 'summary': "Language models can be prompted to reason through problems in a manner that\nsignificantly improves performance. However, \\textit{why} such prompting\nimproves performance is unclear. Recent work showed that using logically\n\\textit{invalid} Chain-of-Thought (CoT) prompting improves performance almost\nas much as logically \\textit{valid} CoT prompting, and that editing CoT prompts\nto replace problem-specific information with abstract information or\nout-of-distribution information typically doesn't harm performance. Critics\nhave responded that these findings are based on too few and too easily solved\ntasks to draw meaningful conclusions. To resolve this dispute, we test whether\nlogically invalid CoT prompts offer the same level of performance gains as\nlogically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed\nBIG-Bench Hard (BBH). We find that the logically \\textit{invalid} reasoning\nprompts do indeed achieve similar performance gains on BBH tasks as logically\nvalid reasoning prompts. We also discover that some CoT prompts used by\nprevious works contain logical errors. This suggests that covariates beyond\nlogically valid reasoning are responsible for performance improvements.", 'author': [{'name': 'Rylan Schaeffer'}, {'name': 'Kateryna Pistunova'}, {'name': 'Samar Khanna'}, {'name': 'Sarthak Consul'}, {'name': 'Sanmi Koyejo'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ICML 2023 Workshop: Knowledge and Logical Reasoning in the Era of\n  Data-driven Learning'}, 'link': [{'@href': 'http://arxiv.org/abs/2307.10573v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2307.10573v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2210.00720v2', 'updated': '2023-01-30T09:15:49Z', 'published': '2022-10-03T05:33:27Z', 'title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'summary': 'We study the task of prompting large-scale language models to perform\nmulti-step reasoning. Existing work shows that when prompted with a chain of\nthoughts (CoT), sequences of short sentences describing intermediate reasoning\nsteps towards a final answer, large language models can generate new reasoning\nchains and predict answers for new inputs. A central question is which\nreasoning examples make the most effective prompts. In this work, we propose\ncomplexity-based prompting, a simple and effective example selection scheme for\nmulti-step reasoning. We show that prompts with higher reasoning complexity,\ni.e., chains with more reasoning steps, achieve substantially better\nperformance on multi-step reasoning tasks over strong baselines. We further\nextend our complexity-based criteria from prompting (selecting inputs) to\ndecoding (selecting outputs), where we sample multiple reasoning chains from\nthe model, then choose the majority of generated answers from complex reasoning\nchains (over simple chains). When used to prompt GPT-3 and Codex, our approach\nsubstantially improves multi-step reasoning accuracy and achieves new\nstate-of-the-art (SOTA) performance on three math benchmarks (GSM8K,\nMultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and\nPenguins), with an average +5.3 and up to +18 accuracy improvements. Compared\nwith existing example selection schemes like manual tuning or retrieval-based\nselection, selection based on reasoning complexity is intuitive, easy to\nimplement, and annotation-efficient. Further results demonstrate the robustness\nof performance gains from complex prompts under format perturbation and\ndistribution shift.', 'author': [{'name': 'Yao Fu'}, {'name': 'Hao Peng'}, {'name': 'Ashish Sabharwal'}, {'name': 'Peter Clark'}, {'name': 'Tushar Khot'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Preprint'}, 'link': [{'@href': 'http://arxiv.org/abs/2210.00720v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2210.00720v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2503.10084v2', 'updated': '2025-06-01T05:23:51Z', 'published': '2025-03-13T06:11:10Z', 'title': 'Why Prompt Design Matters and Works: A Complexity Analysis of Prompt\n  Search Space in LLMs', 'summary': 'Despite the remarkable successes of large language models (LLMs), the\nunderlying Transformer architecture has inherent limitations in handling\ncomplex reasoning tasks. Chain-of-thought (CoT) prompting has emerged as a\npractical workaround, but most CoT-based methods rely on a single, generic\nprompt such as "think step by step", with no task-specific adaptation. These\napproaches expect the model to discover an effective reasoning path on its own,\nforcing it to search through a vast prompt space. In contrast, several studies\nhave explored task-specific prompt designs to boost performance. However, these\ndesigns are typically developed through trial and error, lacking theoretical\ngrounding. As a result, prompt engineering remains largely ad hoc and unguided.\nIn this paper, we provide a theoretical framework that explains why some\nprompts succeed while others fail. We show that prompts function as selectors,\nextracting task-relevant information from the model\'s full hidden state during\nCoT reasoning. Each prompt defines a unique trajectory through the answer\nspace, and the choice of trajectory is crucial for task performance and future\nnavigation within the space. We analyze the complexity of finding optimal\nprompts and characterize the size of the prompt space for a given task. Our\ntheory reveals principles behind effective prompt design and shows that naive\nCoT-using self-guided prompts like "think step by step"-can severely hinder\nperformance. Through experiments, we show that optimal prompt search can lead\nto more than a 50% improvement on reasoning tasks, providing a theoretical\nfoundation for prompt engineering.', 'author': [{'name': 'Xiang Zhang'}, {'name': 'Juntai Cao'}, {'name': 'Jiaqi Wei'}, {'name': 'Chenyu You'}, {'name': 'Dujian Ding'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ACL 2025 main conference'}, 'link': [{'@href': 'http://arxiv.org/abs/2503.10084v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2503.10084v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2405.20902v1', 'updated': '2024-05-31T15:15:04Z', 'published': '2024-05-31T15:15:04Z', 'title': 'Preemptive Answer "Attacks" on Chain-of-Thought Reasoning', 'summary': "Large language models (LLMs) showcase impressive reasoning capabilities when\ncoupled with Chain-of-Thought (CoT) prompting. However, the robustness of this\napproach warrants further investigation. In this paper, we introduce a novel\nscenario termed preemptive answers, where the LLM obtains an answer before\nengaging in reasoning. This situation can arise inadvertently or induced by\nmalicious users by prompt injection attacks. Experiments reveal that preemptive\nanswers significantly impair the model's reasoning capability across various\nCoT methods and a broad spectrum of datasets. To bolster the robustness of\nreasoning, we propose two measures aimed at mitigating this issue to some\nextent.", 'author': [{'name': 'Rongwu Xu'}, {'name': 'Zehan Qi'}, {'name': 'Wei Xu'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': "Accepted to ACL'24 (Findings). Camera-ready version"}, 'link': [{'@href': 'http://arxiv.org/abs/2405.20902v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2405.20902v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CR', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.10200v2', 'updated': '2024-05-23T20:53:59Z', 'published': '2024-02-15T18:55:41Z', 'title': 'Chain-of-Thought Reasoning Without Prompting', 'summary': "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding effectively\nelicits reasoning capabilities from language models, which were previously\nobscured by standard greedy decoding.", 'author': [{'name': 'Xuezhi Wang'}, {'name': 'Denny Zhou'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.10200v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.10200v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2408.08651v2', 'updated': '2024-09-06T01:52:02Z', 'published': '2024-08-16T10:34:50Z', 'title': 'Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of\n  Thought Reasoning', 'summary': 'Language models are known to absorb biases from their training data, leading\nto predictions driven by statistical regularities rather than semantic\nrelevance. We investigate the impact of these biases on answer choice\npreferences in the Massive Multi-Task Language Understanding (MMLU) task. Our\nfindings reveal that differences in learned regularities across answer options\nare predictive of model preferences and mirror human test-taking strategies. To\naddress this issue, we introduce two novel methods: Counterfactual Prompting\nwith Chain of Thought (CoT) and Counterfactual Prompting with Agnostically\nPrimed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with\nCoT alone is insufficient to mitigate bias, our novel Primed Counterfactual\nPrompting with CoT approach effectively reduces the influence of base-rate\nprobabilities while improving overall accuracy. Our results suggest that\nmitigating bias requires a "System-2" like process and that CoT reasoning is\nsusceptible to confirmation bias under some prompting methodologies. Our\ncontributions offer practical solutions for developing more robust and fair\nlanguage models.', 'author': [{'name': 'Kyle Moore'}, {'name': 'Jesse Roberts'}, {'name': 'Thao Pham'}, {'name': 'Douglas Fisher'}], 'link': [{'@href': 'http://arxiv.org/abs/2408.08651v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2408.08651v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2305.16896v1', 'updated': '2023-05-26T13:00:58Z', 'published': '2023-05-26T13:00:58Z', 'title': 'MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of\n  Thought Prompting', 'summary': 'Large language models (LLMs) have achieved impressive performance on various\nreasoning tasks. To further improve the performance, we propose MultiTool-CoT,\na novel framework that leverages chain-of-thought (CoT) prompting to\nincorporate multiple external tools, such as a calculator and a knowledge\nretriever, during the reasoning process. We apply MultiTool-CoT to the Task 2\ndataset of NumGLUE, which requires both numerical reasoning and domain-specific\nknowledge. The experiments show that our method significantly outperforms\nstrong baselines and achieves state-of-the-art performance.', 'author': [{'name': 'Tatsuro Inaba'}, {'name': 'Hirokazu Kiyomaru'}, {'name': 'Fei Cheng'}, {'name': 'Sadao Kurohashi'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ACL2023. Our code is available at\n  https://github.com/InabaTatsuro/MultiTool-CoT'}, 'link': [{'@href': 'http://arxiv.org/abs/2305.16896v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.16896v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2506.01190v1', 'updated': '2025-06-01T21:57:02Z', 'published': '2025-06-01T21:57:02Z', 'title': 'Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance\n  on Culturally-Specific Tasks in Low-Resource Languages', 'summary': 'Large Language Models (LLMs) struggle with culturally-specific reasoning\ntasks, particularly in low-resource languages, hindering their global\napplicability. Addressing this gap is crucial for equitable AI deployment. We\nintroduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting\nstrategy that combines dense vector retrieval of cultural context with explicit\nreasoning sequences. Our extensive experiments on Yoruba proverb interpretation\ndemonstrate that CG-CoT provides significantly higher culturally-aligned\naccuracy and depth than traditional prompting methods, validated through both\nautomated metrics and LLM-based evaluations. Notably, we uncover stark\ndisparities between token-level translation metrics like BLEU and human-judged\ncultural relevance, suggesting a rethinking of evaluation approaches for\nlow-resource NLP.', 'author': {'name': 'Madhavendra Thakur'}, 'link': [{'@href': 'http://arxiv.org/abs/2506.01190v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2506.01190v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2205.10625v3', 'updated': '2023-04-16T22:08:08Z', 'published': '2022-05-21T15:34:53Z', 'title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models', 'summary': 'Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.', 'author': [{'name': 'Denny Zhou'}, {'name': 'Nathanael Sch√§rli'}, {'name': 'Le Hou'}, {'name': 'Jason Wei'}, {'name': 'Nathan Scales'}, {'name': 'Xuezhi Wang'}, {'name': 'Dale Schuurmans'}, {'name': 'Claire Cui'}, {'name': 'Olivier Bousquet'}, {'name': 'Quoc Le'}, {'name': 'Ed Chi'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ICLR 2023'}, 'link': [{'@href': 'http://arxiv.org/abs/2205.10625v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2205.10625v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2412.16533v1', 'updated': '2024-12-21T08:19:42Z', 'published': '2024-12-21T08:19:42Z', 'title': 'Self-guided Knowledgeable Network of Thoughts: Amplifying Reasoning with\n  Large Language Models', 'summary': 'We introduce Knowledgeable Network of Thoughts (kNoT): a prompt scheme that\nadvances the capabilities of large language models (LLMs) beyond existing\nparadigms like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of\nThoughts (GoT). The key innovation of kNoT is the LLM Workflow Template (LWT),\nwhich allows for an executable plan to be specified by LLMs for LLMs. LWT\nallows these plans to be arbitrary networks, where single-step LLM operations\nare nodes, and edges correspond to message passing between these steps.\nFurthermore, LWT supports selection of individual elements through indexing,\nfacilitating kNoT to produce intricate plans where each LLM operation can be\nlimited to elementary operations, greatly enhancing reliability over extended\ntask sequences. We demonstrate that kNoT significantly outperforms the state of\nthe art on six use cases, while reducing the need for extensive prompt\nengineering. For instance, kNoT finds 92% accuracy for sorting 32 numbers over\n12% and 31% for ToT and GoT, while utilizing up to 84.4% and 87.3% less\ntask-specific prompts, respectively.', 'author': [{'name': 'Chao-Chi Chen'}, {'name': 'Chin-Yuan Yeh'}, {'name': 'Hsi-Wen Chen'}, {'name': 'De-Nian Yang'}, {'name': 'Ming-Syan Chen'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'SOTA result over CoT, ToT, GoT'}, 'link': [{'@href': 'http://arxiv.org/abs/2412.16533v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2412.16533v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2310.13571v4', 'updated': '2024-06-06T12:18:56Z', 'published': '2023-10-20T15:09:46Z', 'title': 'Why Can Large Language Models Generate Correct Chain-of-Thoughts?', 'summary': 'This paper delves into the capabilities of large language models (LLMs),\nspecifically focusing on advancing the theoretical comprehension of\nchain-of-thought prompting. We investigate how LLMs can be effectively induced\nto generate a coherent chain of thoughts. To achieve this, we introduce a\ntwo-level hierarchical graphical model tailored for natural language\ngeneration. Within this framework, we establish a compelling geometrical\nconvergence rate that gauges the likelihood of an LLM-generated chain of\nthoughts compared to those originating from the true language. Our findings\nprovide a theoretical justification for the ability of LLMs to produce the\ncorrect sequence of thoughts (potentially) explaining performance gains in\ntasks demanding reasoning skills.', 'author': [{'name': 'Rasul Tutunov'}, {'name': 'Antoine Grosnit'}, {'name': 'Juliusz Ziomek'}, {'name': 'Jun Wang'}, {'name': 'Haitham Bou-Ammar'}], 'link': [{'@href': 'http://arxiv.org/abs/2310.13571v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2310.13571v4', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2501.13117v1', 'updated': '2025-01-20T12:54:57Z', 'published': '2025-01-20T12:54:57Z', 'title': 'MyGO Multiplex CoT: A Method for Self-Reflection in Large Language\n  Models via Double Chain of Thought Thinking', 'summary': 'Recent advancements in large language models (LLMs) have demonstrated their\nimpressive abilities in various reasoning and decision-making tasks. However,\nthe quality and coherence of the reasoning process can still benefit from\nenhanced introspection and self-reflection. In this paper, we introduce\nMultiplex CoT (Chain of Thought), a method that enables LLMs to simulate a form\nof self-review while reasoning, by initiating double Chain of Thought (CoT)\nthinking. Multiplex CoT leverages the power of iterative reasoning, where the\nmodel generates an initial chain of thought and subsequently critiques and\nrefines this reasoning with a second round of thought generation. This\nrecursive approach allows for more coherent, logical, and robust answers,\nimproving the overall decision-making process. We demonstrate how this method\ncan be effectively implemented using simple prompt engineering in existing LLM\narchitectures, achieving an effect similar to that of the Learning-Refinement\nModel (LRM) without the need for additional training. Additionally, we present\na practical guide for implementing the method in Google Colab, enabling easy\nintegration into real-world applications.', 'author': [{'name': 'Shihao Ji'}, {'name': 'Zihui Song'}, {'name': 'Fucheng Zhong'}, {'name': 'Jisen Jia'}, {'name': 'Zhaobo Wu'}, {'name': 'Zheyi Cao'}, {'name': 'Tianhao Xu'}], 'link': [{'@href': 'http://arxiv.org/abs/2501.13117v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2501.13117v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2412.05127v1', 'updated': '2024-12-06T15:35:18Z', 'published': '2024-12-06T15:35:18Z', 'title': 'The Prompt Canvas: A Literature-Based Practitioner Guide for Creating\n  Effective Prompts in Large Language Models', 'summary': "The rise of large language models (LLMs) has highlighted the importance of\nprompt engineering as a crucial technique for optimizing model outputs. While\nexperimentation with various prompting methods, such as Few-shot,\nChain-of-Thought, and role-based techniques, has yielded promising results,\nthese advancements remain fragmented across academic papers, blog posts and\nanecdotal experimentation. The lack of a single, unified resource to\nconsolidate the field's knowledge impedes the progress of both research and\npractical application. This paper argues for the creation of an overarching\nframework that synthesizes existing methodologies into a cohesive overview for\npractitioners. Using a design-based research approach, we present the Prompt\nCanvas, a structured framework resulting from an extensive literature review on\nprompt engineering that captures current knowledge and expertise. By combining\nthe conceptual foundations and practical strategies identified in prompt\nengineering, the Prompt Canvas provides a practical approach for leveraging the\npotential of Large Language Models. It is primarily designed as a learning\nresource for pupils, students and employees, offering a structured introduction\nto prompt engineering. This work aims to contribute to the growing discourse on\nprompt engineering by establishing a unified methodology for researchers and\nproviding guidance for practitioners.", 'author': [{'name': 'Michael Hewing'}, {'name': 'Vincent Leinhos'}], 'link': [{'@href': 'http://arxiv.org/abs/2412.05127v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2412.05127v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2310.14628v2', 'updated': '2023-12-27T13:54:48Z', 'published': '2023-10-23T07:02:20Z', 'title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'summary': 'As large language models (LLMs) have shown effectiveness with different\nprompting methods, such as Chain of Thought, Program of Thought, we find that\nthese methods have formed a great complementarity to each other on math\nreasoning tasks. In this work, we propose XoT, an integrated problem solving\nframework by prompting LLMs with diverse reasoning thoughts. For each question,\nXoT always begins with selecting the most suitable method then executes each\nmethod iteratively. Within each iteration, XoT actively checks the validity of\nthe generated answer and incorporates the feedback from external executors,\nallowing it to dynamically switch among different prompting methods. Through\nextensive experiments on 10 popular math reasoning datasets, we demonstrate the\neffectiveness of our proposed approach and thoroughly analyze the strengths of\neach module. Moreover, empirical results suggest that our framework is\northogonal to recent work that makes improvements on single reasoning methods\nand can further generalise to logical reasoning domain. By allowing method\nswitching, XoT provides a fresh perspective on the collaborative integration of\ndiverse reasoning thoughts in a unified framework. The code is available at\nhttps://github.com/tengxiaoliu/XoT.', 'author': [{'name': 'Tengxiao Liu'}, {'name': 'Qipeng Guo'}, {'name': 'Yuqing Yang'}, {'name': 'Xiangkun Hu'}, {'name': 'Yue Zhang'}, {'name': 'Xipeng Qiu'}, {'name': 'Zheng Zhang'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'EMNLP 2023'}, 'link': [{'@href': 'http://arxiv.org/abs/2310.14628v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2310.14628v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2504.04373v1', 'updated': '2025-04-06T06:02:28Z', 'published': '2025-04-06T06:02:28Z', 'title': 'StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style\n  Transformation', 'summary': 'Prompt Recovery, reconstructing prompts from the outputs of large language\nmodels (LLMs), has grown in importance as LLMs become ubiquitous. Most users\naccess LLMs through APIs without internal model weights, relying only on\noutputs and logits, which complicates recovery. This paper explores a unique\nprompt recovery task focused on reconstructing prompts for style transfer and\nrephrasing, rather than typical question-answering. We introduce a dataset\ncreated with LLM assistance, ensuring quality through multiple techniques, and\ntest methods like zero-shot, few-shot, jailbreak, chain-of-thought,\nfine-tuning, and a novel canonical-prompt fallback for poor-performing cases.\nOur results show that one-shot and fine-tuning yield the best outcomes but\nhighlight flaws in traditional sentence similarity metrics for evaluating\nprompt recovery. Contributions include (1) a benchmark dataset, (2)\ncomprehensive experiments on prompt recovery strategies, and (3) identification\nof limitations in current evaluation metrics, all of which advance general\nprompt recovery research, where the structure of the input prompt is\nunrestricted.', 'author': [{'name': 'Shenyang Liu'}, {'name': 'Yang Gao'}, {'name': 'Shaoyan Zhai'}, {'name': 'Liqiang Wang'}], 'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '10.1109/BigData62323.2024.10825143'}, 'link': [{'@title': 'doi', '@href': 'http://dx.doi.org/10.1109/BigData62323.2024.10825143', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2504.04373v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2504.04373v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '2024 IEEE International Conference on Big Data (BigData)'}, 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2306.06427v3', 'updated': '2024-06-03T14:59:11Z', 'published': '2023-06-10T12:42:36Z', 'title': 'Boosting Language Models Reasoning with Chain-of-Knowledge Prompting', 'summary': "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex\nreasoning tasks, which aims at designing a simple prompt like ``Let's think\nstep by step'' or multiple in-context exemplars with well-designed rationales\nto elicit Large Language Models (LLMs) to generate intermediate reasoning\nsteps. However, the generated rationales often come with mistakes, making\nunfactual and unfaithful reasoning chains. To mitigate this brittleness, we\npropose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting\nLLMs to generate explicit pieces of knowledge evidence in the form of structure\ntriple. This is inspired by our human behaviors, i.e., we can draw a mind map\nor knowledge map as the reasoning evidence in the brain before answering a\ncomplex question. Benefiting from CoK, we additionally introduce a\nF^2-Verification method to estimate the reliability of the reasoning chains in\nterms of factuality and faithfulness. For the unreliable response, the wrong\nevidence can be indicated to prompt the LLM to rethink. Extensive experiments\ndemonstrate that our method can further improve the performance of commonsense,\nfactual, symbolic, and arithmetic reasoning tasks.", 'author': [{'name': 'Jianing Wang'}, {'name': 'Qiushi Sun'}, {'name': 'Xiang Li'}, {'name': 'Ming Gao'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ACL 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2306.06427v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2306.06427v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2408.15205v3', 'updated': '2024-11-22T17:27:31Z', 'published': '2024-08-27T17:06:22Z', 'title': 'Leveraging Hallucinations to Reduce Manual Prompt Dependency in\n  Promptable Segmentation', 'summary': 'Promptable segmentation typically requires instance-specific manual prompts\nto guide the segmentation of each desired object. To minimize such a need,\ntask-generic promptable segmentation has been introduced, which employs a\nsingle task-generic prompt to segment various images of different objects in\nthe same task. Current methods use Multimodal Large Language Models (MLLMs) to\nreason detailed instance-specific prompts from a task-generic prompt for\nimproving segmentation accuracy. The effectiveness of this segmentation heavily\ndepends on the precision of these derived prompts. However, MLLMs often suffer\nhallucinations during reasoning, resulting in inaccurate prompting. While\nexisting methods focus on eliminating hallucinations to improve a model, we\nargue that MLLM hallucinations can reveal valuable contextual insights when\nleveraged correctly, as they represent pre-trained large-scale knowledge beyond\nindividual images. In this paper, we utilize hallucinations to mine\ntask-related information from images and verify its accuracy for enhancing\nprecision of the generated prompts. Specifically, we introduce an iterative\nPrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a\nmask generator.The prompt generator uses a multi-scale chain of thought\nprompting, initially exploring hallucinations for extracting extended\ncontextual knowledge on a test image.These hallucinations are then reduced to\nformulate precise instance-specific prompts, directing the mask generator to\nproduce masks that are consistent with task semantics by mask semantic\nalignment. The generated masks iteratively induce the prompt generator to focus\nmore on task-relevant image areas and reduce irrelevant hallucinations,\nresulting jointly in better prompts and masks. Experiments on 5 benchmarks\ndemonstrate the effectiveness of ProMaC. Code given in\nhttps://lwpyh.github.io/ProMaC/.', 'author': [{'name': 'Jian Hu'}, {'name': 'Jiayi Lin'}, {'name': 'Junchi Yan'}, {'name': 'Shaogang Gong'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'NeurIPS 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2408.15205v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2408.15205v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2405.01249v1', 'updated': '2024-05-02T12:52:23Z', 'published': '2024-05-02T12:52:23Z', 'title': 'Prompt engineering paradigms for medical applications: scoping review\n  and recommendations for better practices', 'summary': 'Prompt engineering is crucial for harnessing the potential of large language\nmodels (LLMs), especially in the medical domain where specialized terminology\nand phrasing is used. However, the efficacy of prompt engineering in the\nmedical domain remains to be explored. In this work, 114 recent studies\n(2022-2024) applying prompt engineering in medicine, covering prompt learning\n(PL), prompt tuning (PT), and prompt design (PD) are reviewed. PD is the most\nprevalent (78 articles). In 12 papers, PD, PL, and PT terms were used\ninterchangeably. ChatGPT is the most commonly used LLM, with seven papers using\nit for processing sensitive clinical data. Chain-of-Thought emerges as the most\ncommon prompt engineering technique. While PL and PT articles typically provide\na baseline for evaluating prompt-based approaches, 64% of PD studies lack\nnon-prompt-related baselines. We provide tables and figures summarizing\nexisting work, and reporting recommendations to guide future research\ncontributions.', 'author': [{'name': 'Jamil Zaghir'}, {'name': 'Marco Naguib'}, {'name': 'Mina Bjelogrlic'}, {'name': 'Aur√©lie N√©v√©ol'}, {'name': 'Xavier Tannier'}, {'name': 'Christian Lovis'}], 'link': [{'@href': 'http://arxiv.org/abs/2405.01249v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2405.01249v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.06918v2', 'updated': '2024-06-26T05:47:52Z', 'published': '2024-02-10T09:51:03Z', 'title': 'Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to\n  Searching for the Most Promising Intermediate Thought', 'summary': "To improve the ability of the large language model (LLMs) to tackle complex\nreasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs\nto reason step-by-step, enabling problem solving from simple to complex.\nState-of-the-art methods for generating such a chain involve interactive\ncollaboration, where the learner generates candidate intermediate thoughts,\nevaluated by the LLM, guiding the generation of subsequent thoughts. However, a\nwidespread yet understudied problem is that the evaluation from the LLM is\ntypically noisy and unreliable, potentially misleading the generation process\nin selecting promising intermediate thoughts. In this paper, motivated by\nVapnik's principle, we use pairwise-comparison evaluation instead of point-wise\nscoring to search for promising intermediate thoughts with the noisy feedback\nfrom the LLM. In each round, we randomly pair intermediate thoughts and\ndirectly prompt the LLM to select the more promising one from each pair,\nallowing us to identify the most promising thoughts through an iterative\nprocess. To further alleviate the noise in the comparison, we incorporate\ntechniques from ensemble learning and dueling bandits, proposing two variants\nof the algorithm. Experiments on three real-world tasks demonstrate the\neffectiveness of our proposed algorithm and verify the rationale of the\npairwise comparison mechanism.", 'author': [{'name': 'Zhen-Yu Zhang'}, {'name': 'Siwei Han'}, {'name': 'Huaxiu Yao'}, {'name': 'Gang Niu'}, {'name': 'Masashi Sugiyama'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ICML 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.06918v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.06918v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2503.22853v1', 'updated': '2025-03-28T20:15:24Z', 'published': '2025-03-28T20:15:24Z', 'title': 'Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought\n  Prompting: Pedagogical Strategies for Machines', 'summary': 'This study evaluates the baseline capabilities of Large Language Models\n(LLMs) like ChatGPT, Claude, and Gemini to learn concepts in music theory\nthrough in-context learning and chain-of-thought prompting. Using carefully\ndesigned prompts (in-context learning) and step-by-step worked examples\n(chain-of-thought prompting), we explore how LLMs can be taught increasingly\ncomplex material and how pedagogical strategies for human learners translate to\neducating machines. Performance is evaluated using questions from an official\nCanadian Royal Conservatory of Music (RCM) Level 6 examination, which covers a\ncomprehensive range of topics, including interval and chord identification, key\ndetection, cadence classification, and metrical analysis. Additionally, we\nevaluate the suitability of various music encoding formats for these tasks\n(ABC, Humdrum, MEI, MusicXML). All experiments were run both with and without\ncontextual prompts. Results indicate that without context, ChatGPT with MEI\nperforms the best at 52%, while with context, Claude with MEI performs the best\nat 75%. Future work will further refine prompts and expand to cover more\nadvanced music theory concepts. This research contributes to the broader\nunderstanding of teaching LLMs and has applications for educators, students,\nand developers of AI music tools alike.', 'author': [{'name': 'Liam Pond'}, {'name': 'Ichiro Fujinaga'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '11 pages, 4 figures, 3 tables. Published in Volume 1 of the\n  Proceedings of the 17th International Conference on Computer Supported Music\n  Education (CSME 2025). Presented on 3 April 2025 in Porto, Portugal'}, 'link': [{'@href': 'http://arxiv.org/abs/2503.22853v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2503.22853v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2412.19707v1', 'updated': '2024-12-27T16:02:34Z', 'published': '2024-12-27T16:02:34Z', 'title': 'Toward Adaptive Reasoning in Large Language Models with Thought Rollback', 'summary': "Large language models (LLMs) have been routinely used to solve various tasks\nusing step-by-step reasoning. However, the structure of intermediate reasoning\nsteps, or thoughts, is rigid and unidirectional, such as chains, trees, or\nacyclic-directed graphs. Consequently, the resulting inflexible and\nforward-only reasoning may not address challenging tasks and fail when the LLM\nfrequently gives false responses, i.e., ``hallucinations''. This paper proposes\na new reasoning framework, called Thought Rollback (TR), allowing LLMs to\nadaptively build thought structure while maintaining effective reasoning toward\nproblem-solving under ``hallucinations''. The core mechanism of TR is rolling\nback thoughts, which allows LLMs to perform error analysis on thoughts, and\nthus roll back to any previously mistaken thought for revision. Subsequently,\nby including such trial-and-error in the prompt to guide the LLM, each rollback\nleads to one more reliable reasoning path. Therefore, starting with a simple\nprompt without human annotations, LLM with TR adaptively and gradually explores\nthoughts for a correct solution. Comprehensive experiments on mathematical\nproblems and multi-task reasoning demonstrate the state-of-the-art performance\nof TR in terms of problem-solving rate and interaction cost. For instance, the\nsolving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH\ndataset.", 'author': [{'name': 'Sijia Chen'}, {'name': 'Baochun Li'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ICML 2024 camera-ready version with 24 pages and 12 figures. Code\n  repo with all prompts:\n  https://github.com/iQua/llmpebase/tree/main/examples/ThoughtRollback'}, 'link': [{'@href': 'http://arxiv.org/abs/2412.19707v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2412.19707v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2503.02003v3', 'updated': '2025-05-22T20:17:45Z', 'published': '2025-03-03T19:26:04Z', 'title': 'HoT: Highlighted Chain of Thought for Referencing Supporting Facts from\n  Inputs', 'summary': 'An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct.', 'author': [{'name': 'Tin Nguyen'}, {'name': 'Logan Bolton'}, {'name': 'Mohammad Reza Taesiri'}, {'name': 'Anh Totti Nguyen'}], 'link': [{'@href': 'http://arxiv.org/abs/2503.02003v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2503.02003v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2501.04671v2', 'updated': '2025-04-08T17:09:59Z', 'published': '2025-01-08T18:31:16Z', 'title': 'Retrieval-Based Interleaved Visual Chain-of-Thought in Real-World\n  Driving Scenarios', 'summary': 'While chain-of-thought (CoT) prompting improves reasoning in large language\nmodels, its effectiveness in vision-language models (VLMs) remains limited due\nto over-reliance on textual cues and memorized knowledge. To investigate the\nvisual reasoning capabilities of VLMs in complex real-world scenarios, we\nintroduce DrivingVQA, a visual question answering dataset derived from driving\ntheory exams, which contains 3,931 multiple-choice problems with expert-written\nexplanations and grounded entities relevant to the reasoning process.\nLeveraging this dataset, we propose RIV-CoT, a Retrieval-Based Interleaved\nVisual Chain-of-Thought method that enables VLMs to reason using visual crops\ncorresponding to these relevant entities. Our experiments demonstrate that\nRIV-CoT improves answer accuracy by 3.1% and reasoning accuracy by 4.6% over\nvanilla CoT prompting. Furthermore, we demonstrate that our method effectively\nscales to the larger A-OKVQA reasoning dataset by leveraging automatically\ngenerated pseudo-labels, outperforming CoT prompting.', 'author': [{'name': 'Charles Corbi√®re'}, {'name': 'Simon Roburin'}, {'name': 'Syrielle Montariol'}, {'name': 'Antoine Bosselut'}, {'name': 'Alexandre Alahi'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Project page: https://vita-epfl.github.io/DrivingVQA'}, 'link': [{'@href': 'http://arxiv.org/abs/2501.04671v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2501.04671v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2501.18645v2', 'updated': '2025-02-03T15:51:11Z', 'published': '2025-01-29T13:21:09Z', 'title': 'Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A\n  Comprehensive Approach to Explainable Large Language Models', 'summary': 'Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to\nprovide step-by-step rationales, improving performance on complex tasks.\nDespite its benefits, vanilla CoT often fails to fully verify intermediate\ninferences and can produce misleading explanations. In this work, we propose\nLayered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that\nsystematically segments the reasoning process into multiple layers, each\nsubjected to external checks and optional user feedback. We expand on the key\nconcepts, present three scenarios -- medical triage, financial risk assessment,\nand agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT\nin terms of transparency, correctness, and user engagement. By integrating\nreferences from recent arXiv papers on interactive explainability, multi-agent\nframeworks, and agent-based collaboration, we illustrate how Layered-CoT paves\nthe way for more reliable and grounded explanations in high-stakes domains.', 'author': {'name': 'Manish Sanwal'}, 'link': [{'@href': 'http://arxiv.org/abs/2501.18645v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2501.18645v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2405.13872v2', 'updated': '2024-05-29T02:24:36Z', 'published': '2024-05-22T17:56:51Z', 'title': 'Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal\n  Large Language Models', 'summary': 'Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models', 'author': [{'name': 'Qiji Zhou'}, {'name': 'Ruochen Zhou'}, {'name': 'Zike Hu'}, {'name': 'Panzhong Lu'}, {'name': 'Siyang Gao'}, {'name': 'Yue Zhang'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Correct the case title'}, 'link': [{'@href': 'http://arxiv.org/abs/2405.13872v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2405.13872v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2309.12669v1', 'updated': '2023-09-22T07:26:17Z', 'published': '2023-09-22T07:26:17Z', 'title': 'HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text\n  Hybrid Question Answering', 'summary': 'Answering numerical questions over hybrid contents from the given tables and\ntext(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)\nhave gained significant attention in the NLP community. With the emergence of\nlarge language models, In-Context Learning and Chain-of-Thought prompting have\nbecome two particularly popular research topics in this field. In this paper,\nwe introduce a new prompting strategy called Hybrid prompt strategy and\nRetrieval of Thought for TextTableQA. Through In-Context Learning, we prompt\nthe model to develop the ability of retrieval thinking when dealing with hybrid\ndata. Our method achieves superior performance compared to the fully-supervised\nSOTA on the MultiHiertt dataset in the few-shot setting.', 'author': [{'name': 'Tongxu Luo'}, {'name': 'Fangyu Lei'}, {'name': 'Jiahe Lei'}, {'name': 'Weihao Liu'}, {'name': 'Shihu He'}, {'name': 'Jun Zhao'}, {'name': 'Kang Liu'}], 'link': [{'@href': 'http://arxiv.org/abs/2309.12669v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2309.12669v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2309.08008v1', 'updated': '2023-09-14T19:35:00Z', 'published': '2023-09-14T19:35:00Z', 'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\n  Models in Zero-Shot Clinical Natural Language Processing', 'summary': 'Large language models (LLMs) have shown remarkable capabilities in Natural\nLanguage Processing (NLP), especially in domains where labeled data is scarce\nor expensive, such as clinical domain. However, to unlock the clinical\nknowledge hidden in these LLMs, we need to design effective prompts that can\nguide them to perform specific clinical NLP tasks without any task-specific\ntraining data. This is known as in-context learning, which is an art and\nscience that requires understanding the strengths and weaknesses of different\nLLMs and prompt engineering approaches. In this paper, we present a\ncomprehensive and systematic experimental study on prompt engineering for five\nclinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence\nExtraction, Coreference Resolution, Medication Status Extraction, and\nMedication Attribute Extraction. We assessed the prompts proposed in recent\nliterature, including simple prefix, simple cloze, chain of thought, and\nanticipatory prompts, and introduced two new types of prompts, namely heuristic\nprompting and ensemble prompting. We evaluated the performance of these prompts\non three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted\nzero-shot prompting with few-shot prompting, and provide novel insights and\nguidelines for prompt engineering for LLMs in clinical NLP. To the best of our\nknowledge, this is one of the first works on the empirical evaluation of\ndifferent prompt engineering approaches for clinical NLP in this era of\ngenerative AI, and we hope that it will inspire and inform future research in\nthis area.', 'author': [{'name': 'Sonish Sivarajkumar'}, {'name': 'Mark Kelley'}, {'name': 'Alyssa Samolyk-Mazzanti'}, {'name': 'Shyam Visweswaran'}, {'name': 'Yanshan Wang'}], 'link': [{'@href': 'http://arxiv.org/abs/2309.08008v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2309.08008v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2308.09687v4', 'updated': '2024-02-06T18:00:18Z', 'published': '2023-08-18T17:29:23Z', 'title': 'Graph of Thoughts: Solving Elaborate Problems with Large Language Models', 'summary': 'We introduce Graph of Thoughts (GoT): a framework that advances prompting\ncapabilities in large language models (LLMs) beyond those offered by paradigms\nsuch as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information generated by an LLM as\nan arbitrary graph, where units of information ("LLM thoughts") are vertices,\nand edges correspond to dependencies between these vertices. This approach\nenables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback\nloops. We illustrate that GoT offers advantages over state of the art on\ndifferent tasks, for example increasing the quality of sorting by 62% over ToT,\nwhile simultaneously reducing costs by >31%. We ensure that GoT is extensible\nwith new thought transformations and thus can be used to spearhead new\nprompting schemes. This work brings the LLM reasoning closer to human thinking\nor brain mechanisms such as recurrence, both of which form complex networks.', 'author': [{'name': 'Maciej Besta'}, {'name': 'Nils Blach'}, {'name': 'Ales Kubicek'}, {'name': 'Robert Gerstenberger'}, {'name': 'Michal Podstawski'}, {'name': 'Lukas Gianinazzi'}, {'name': 'Joanna Gajda'}, {'name': 'Tomasz Lehmann'}, {'name': 'Hubert Niewiadomski'}, {'name': 'Piotr Nyczyk'}, {'name': 'Torsten Hoefler'}], 'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '10.1609/aaai.v38i16.29720'}, 'link': [{'@title': 'doi', '@href': 'http://dx.doi.org/10.1609/aaai.v38i16.29720', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2308.09687v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2308.09687v4', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': "Proceedings of the AAAI Conference on Artificial Intelligence 2024\n  (AAAI'24)"}, 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2212.10001v2', 'updated': '2023-06-01T05:38:00Z', 'published': '2022-12-20T05:20:54Z', 'title': 'Towards Understanding Chain-of-Thought Prompting: An Empirical Study of\n  What Matters', 'summary': "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.", 'author': [{'name': 'Boshi Wang'}, {'name': 'Sewon Min'}, {'name': 'Xiang Deng'}, {'name': 'Jiaming Shen'}, {'name': 'You Wu'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Huan Sun'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'ACL-23 Camera Ready. Code and model input/output are available at\n  https://github.com/sunlab-osu/Understanding-CoT'}, 'link': [{'@href': 'http://arxiv.org/abs/2212.10001v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2212.10001v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2305.14564v1', 'updated': '2023-05-23T23:06:04Z', 'published': '2023-05-23T23:06:04Z', 'title': 'PEARL: Prompting Large Language Models to Plan and Execute Actions Over\n  Long Documents', 'summary': 'Strategies such as chain-of-thought prompting improve the performance of\nlarge language models (LLMs) on complex reasoning tasks by decomposing input\nexamples into intermediate steps. However, it remains unclear how to apply such\nmethods to reason over long input documents, in which both the decomposition\nand the output of each intermediate step are non-trivial to obtain. In this\nwork, we propose PEARL, a prompting framework to improve reasoning over long\ndocuments, which consists of three stages: action mining, plan formulation, and\nplan execution. More specifically, given a question about a long document,\nPEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,\nFIND_EVENT, FIND_RELATION) and then executes them over the document to obtain\nthe answer. Each stage of PEARL is implemented via zero-shot or few-shot\nprompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate\nPEARL on a challenging subset of the QuALITY dataset, which contains questions\nthat require complex reasoning over long narrative texts. PEARL outperforms\nzero-shot and chain-of-thought prompting on this dataset, and ablation\nexperiments show that each stage of PEARL is critical to its performance.\nOverall, PEARL is a first step towards leveraging LLMs to reason over long\ndocuments.', 'author': [{'name': 'Simeng Sun'}, {'name': 'Yang Liu'}, {'name': 'Shuohang Wang'}, {'name': 'Chenguang Zhu'}, {'name': 'Mohit Iyyer'}], 'link': [{'@href': 'http://arxiv.org/abs/2305.14564v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.14564v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2309.16621v1', 'updated': '2023-09-28T17:21:33Z', 'published': '2023-09-28T17:21:33Z', 'title': 'Stress Testing Chain-of-Thought Prompting for Large Language Models', 'summary': 'This report examines the effectiveness of Chain-of-Thought (CoT) prompting in\nimproving the multi-step reasoning abilities of large language models (LLMs).\nInspired by previous studies \\cite{Min2022RethinkingWork}, we analyze the\nimpact of three types of CoT prompt perturbations, namely CoT order, CoT\nvalues, and CoT operators on the performance of GPT-3 on various tasks. Our\nfindings show that incorrect CoT prompting leads to poor performance on\naccuracy metrics. Correct values in the CoT is crucial for predicting correct\nanswers. Moreover, incorrect demonstrations, where the CoT operators or the CoT\norder are wrong, do not affect the performance as drastically when compared to\nthe value based perturbations. This research deepens our understanding of CoT\nprompting and opens some new questions regarding the capability of LLMs to\nlearn reasoning in context.', 'author': [{'name': 'Aayush Mishra'}, {'name': 'Karan Thakkar'}], 'link': [{'@href': 'http://arxiv.org/abs/2309.16621v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2309.16621v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2304.13007v4', 'updated': '2024-08-02T14:18:51Z', 'published': '2023-04-25T17:27:37Z', 'title': 'Answering Questions by Meta-Reasoning over Multiple Chains of Thought', 'summary': 'Modern systems for multi-hop question answering (QA) typically break\nquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),\nbefore arriving at a final answer. Often, multiple chains are sampled and\naggregated through a voting mechanism over the final answers, but the\nintermediate steps themselves are discarded. While such approaches improve\nperformance, they do not consider the relations between intermediate steps\nacross chains and do not provide a unified explanation for the predicted\nanswer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts\nlarge language models to meta-reason over multiple chains of thought, rather\nthan aggregating their answers. MCR examines different reasoning chains, mixes\ninformation between them and selects the most relevant facts in generating an\nexplanation and predicting the answer. MCR outperforms strong baselines on 7\nmulti-hop QA datasets. Moreover, our analysis reveals that MCR explanations\nexhibit high quality, enabling humans to verify its answers.', 'author': [{'name': 'Ori Yoran'}, {'name': 'Tomer Wolfson'}, {'name': 'Ben Bogin'}, {'name': 'Uri Katz'}, {'name': 'Daniel Deutch'}, {'name': 'Jonathan Berant'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': "Accepted for publication in The 2023 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP 2023). Author's final version"}, 'link': [{'@href': 'http://arxiv.org/abs/2304.13007v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2304.13007v4', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2409.15359v2', 'updated': '2024-10-01T20:24:38Z', 'published': '2024-09-17T23:47:20Z', 'title': 'Watch Your Steps: Observable and Modular Chains of Thought', 'summary': 'We propose a variant of chain of thought (CoT) prompting called Program Trace\nPrompting that makes explanations more observable while preserving the power,\ngenerality and flexibility of CoT. In our approach, few-shot CoT demonstrations\nare wrapped in a formal syntax based on Python, and each prompt: identifies and\nnames steps; defines the input/output behavior of steps; and replaces CoT\nexplanations of in-context examples with chains of these formalized steps on\nthe same examples. Program Trace Prompting is applicable to many tasks,\nachieving strong results on the 23 diverse tasks in the BIG-Bench Hard\nbenchmark. More importantly, by instrumenting explanations in this way, we\nenable new types of analysis. In particular, we identify "non-local errors"\n(which correspond to incorrectly learning the reasoning method illustrated in\nthe demonstrations) as an unaddressed issue in CoT learning, and we present\nmethods for verifying the modularity of steps in a CoT explanation.', 'author': [{'name': 'Cassandra A. Cohen'}, {'name': 'William W. Cohen'}], 'link': [{'@href': 'http://arxiv.org/abs/2409.15359v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2409.15359v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2306.03799v2', 'updated': '2024-03-28T03:23:59Z', 'published': '2023-06-06T15:43:16Z', 'title': 'Prompt Space Optimizing Few-shot Reasoning Success with Large Language\n  Models', 'summary': 'Prompt engineering is an essential technique for enhancing the abilities of\nlarge language models (LLMs) by providing explicit and specific instructions.\nIt enables LLMs to excel in various tasks, such as arithmetic reasoning,\nquestion answering, summarization, relation extraction, machine translation,\nand sentiment analysis. Researchers have been actively exploring different\nprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and\nIn-context learning. However, an unresolved problem arises from the fact that\ncurrent approaches lack a solid mathematical solution for determining optimal\nprompts. To address this issue in prompt engineering, we propose a new and\neffective approach called Prompt Space. Our methodology utilizes text\nembeddings to obtain basis vectors by matrix decomposition, and then constructs\na space for representing all prompts. Prompt Space significantly outperforms\nstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,\nwithout the help of the CoT method and the prompt "Let\'s think step by step",\nPrompt Space shows superior performance over the few-shot method. Overall, our\napproach provides a robust and effective mathematical framework for selecting\nsimple and effective prompts. This advancement marks a significant step towards\nimproving prompt engineering for a wide variety of applications in LLMs. Our\ncode is publicly available at\n\\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}', 'author': [{'name': 'Fobo Shi'}, {'name': 'Peijun Qing'}, {'name': 'Dong Yang'}, {'name': 'Nan Wang'}, {'name': 'Youbo Lei'}, {'name': 'Haonan Lu'}, {'name': 'Xiaodong Lin'}, {'name': 'Duantengchuan Li'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Natural language processing (NLP)'}, 'link': [{'@href': 'http://arxiv.org/abs/2306.03799v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2306.03799v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}, {'id': 'http://arxiv.org/abs/2501.13122v1', 'updated': '2025-01-21T03:52:54Z', 'published': '2025-01-21T03:52:54Z', 'title': 'Zero-Shot Verification-guided Chain of Thoughts', 'summary': "Previous works have demonstrated the effectiveness of Chain-of-Thought (COT)\nprompts and verifiers in guiding Large Language Models (LLMs) through the space\nof reasoning. However, most such studies either use a fine-tuned verifier or\nrely on manually handcrafted few-shot examples. In contrast, in this paper, we\nfocus on LLM-based self-verification of self-generated reasoning steps via COT\nprompts in a completely zero-shot regime. To explore this setting, we design a\nnew zero-shot prompt, which we call COT STEP, to aid zero-shot decomposition of\nreasoning steps and design two new zero-shot prompts for LLM-based verifiers.\nWe evaluate the verifiers' ability to classify the correctness of reasoning\nchains and explore different ways to use verifier scores in guiding reasoning\nfor various mathematical and commonsense reasoning tasks with different LLMs.", 'author': [{'name': 'Jishnu Ray Chowdhury'}, {'name': 'Cornelia Caragea'}], 'link': [{'@href': 'http://arxiv.org/abs/2501.13122v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2501.13122v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2203.08383v3', 'updated': '2022-10-23T17:44:01Z', 'published': '2022-03-16T04:12:20Z', 'title': 'Iteratively Prompt Pre-trained Language Models for Chain of Thought', 'summary': 'While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex & multi-step reasoning. Similar to how humans develop a\n"chain of thought" for these tasks, how can we equip PLMs with such abilities?\nIn this work, we explore an iterative prompting framework, a new prompting\nparadigm which progressively elicits relevant knowledge from PLMs for\nmulti-step inference. We identify key limitations of existing prompting\nmethods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step\'s contexts. Experiments on three datasets involving multi-step\nreasoning show the effectiveness of the iterative scheme and the context-aware\nprompter design.', 'author': [{'name': 'Boshi Wang'}, {'name': 'Xiang Deng'}, {'name': 'Huan Sun'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'EMNLP-2022 Camera-Ready. Code will be available at\n  https://github.com/sunlab-osu/IterPrompt'}, 'link': [{'@href': 'http://arxiv.org/abs/2203.08383v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2203.08383v3', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}}][0m[32;1m[1;3m
Invoking: `PaperHTMLExtractor` with `http://arxiv.org/abs/2201.11903v6`


[0m<!DOCTYPE html>
<html lang="en">
  <head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/static/base/1.0.1/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/static/base/1.0.1/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/static/base/1.0.1/images/icons/favicon-16x16.png">
<link rel="manifest" href="/static/base/1.0.1/images/icons/site.webmanifest">
<link rel="mask-icon" href="/static/base/1.0.1/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="/static/base/1.0.1/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title> | arXiv e-print repository</title>
<script defer src="/static/base/1.0.1/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="/static/base/1.0.1/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="/static/base/1.0.1/js/notification.js"></script>
  </head>
  <body>
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
<!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="/static/base/1.0.1/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="/static/base/1.0.1/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
    <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
<a href="https://arxiv.org/login">Login</a>    </div>
</div>  </header>
  <main class="container" id="main-container">
<h1>No HTML for '2201.11903v6'</h1>
<p>HTML is not available for the source.</p>
<p>This could be due to the source files not being HTML, LaTeX, or a conversion failure.</p>
<p>If you are an author, learn how you can help <a href="https://info.arxiv.org/about/accessibility_html_error_messages.html">HTML conversions for your papers</a>.</p>  </main>
  <footer>
<div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>  </footer>
  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>
  </body>
</html>
[33;1m[1;3mNone[0m[32;1m[1;3mHere are five of the most influential recent papers on Chain-of-Thought (CoT) prompting in large language models.  They include the original CoT work, key improvements (self-consistency, least-to-most), and methods for automatically constructing and refining CoT demonstrations.

1. ‚ÄúChain-of-Thought Prompting Elicits Reasoning in Large Language Models‚Äù  
   ‚Ä¢ Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou  
   ‚Ä¢ arXiv:2201.11903 (January 2022; updated v6 Jan 2023)  
   ‚Ä¢ Introduces CoT prompting: by prepending a few examples showing step-by-step reasoning, very large LMs (e.g., PaLM-540B) dramatically improve on arithmetic, commonsense, and symbolic reasoning benchmarks (e.g., GSM8K).  
   ‚Ä¢ Link: https://arxiv.org/abs/2201.11903  

2. ‚ÄúSelf-Consistency Improves Chain-of-Thought Reasoning in Language Models‚Äù  
   ‚Ä¢ Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou  
   ‚Ä¢ arXiv:2203.11171 (ICLR 2023)  
   ‚Ä¢ Replaces greedy decoding of a single CoT with sampling many reasoning chains and ‚Äúvoting‚Äù on the most consistent answer.  Achieves large gains (+10‚Äì18 points) across arithmetic and commonsense benchmarks (GSM8K, SVAMP, AQuA).  
   ‚Ä¢ Link: https://arxiv.org/abs/2203.11171  

3. ‚ÄúLeast-to-Most Prompting Enables Complex Reasoning in Large Language Models‚Äù  
   ‚Ä¢ Denny Zhou, Nathana√´l Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi  
   ‚Ä¢ arXiv:2205.10625 (ICLR 2023)  
   ‚Ä¢ Decomposes hard problems into a sequence of simpler sub-questions: prompt the model first on an easy subtask, feed its answer back in to solve the next subtask, and so on.  On SCAN compositional generalization and math benchmarks it far outperforms vanilla CoT.  
   ‚Ä¢ Link: https://arxiv.org/abs/2205.10625  

4. ‚ÄúSynthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models‚Äù  
   ‚Ä¢ Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen  
   ‚Ä¢ arXiv:2302.00618 (2023)  
   ‚Ä¢ Uses an existing CoT-prompted LLM to synthesize new high-quality chain-of-thought examples automatically, alternating between generating questions from reasoning chains and refining reasoning chains for questions.  Outperforms manual few-shot CoT across numerical, symbolic, and algorithmic reasoning tasks.  
   ‚Ä¢ Link: https://arxiv.org/abs/2302.00618  

5. ‚ÄúReprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling‚Äù  
   ‚Ä¢ Weijia Xu, Andrzej Banburski-Fahey, Nebojsa Jojic  
   ‚Ä¢ arXiv:2305.09993 (ICML 2024)  
   ‚Ä¢ Introduces an iterative Gibbs-sampling approach to automatically discover effective CoT prompt templates (‚Äúrecipes‚Äù) by treating them as latent variables and sampling new prompts conditioned on training examples.  Outperforms human-written CoT and other prompt-optimization methods on 20 reasoning tasks.  
   ‚Ä¢ Link: https://arxiv.org/abs/2305.09993  

These five papers chart the development of Chain-of-Thought prompting‚Äîfrom the basic idea of exposing step-by-step rationales, to improvements in decoding (self-consistency), structured decomposition (least-to-most), and methods for automatically generating or optimizing CoT prompts.[0m

[1m> Finished chain.[0m
